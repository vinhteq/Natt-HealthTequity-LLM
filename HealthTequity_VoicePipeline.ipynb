{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinhteq/Natt-HealthTequity-LLM/blob/main/HealthTequity_VoicePipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XBAdMOleww-o",
      "metadata": {
        "id": "XBAdMOleww-o"
      },
      "source": [
        "# 🩺 HealthTequity Voice Processing Pipeline\n",
        "\n",
        "## 📘 Introduction\n",
        "This notebook demonstrates an end-to-end **Voice-to-Insight** pipeline developed for the **HealthTequity Case Study**.  \n",
        "It processes Spanish medical speech into actionable insights through **transcription**, **translation**, **LLM-based reasoning**, and **automated evaluation** using **WER**, **CER**, and **SER** metrics.  \n",
        "\n",
        "The system connects **speech understanding (ASR)** with **data analytics (LLM)** and **speech synthesis (TTS)**, forming a reproducible and modular workflow for healthcare data analysis.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧩 Companion Notebooks\n",
        "Two supporting notebooks generate the datasets used in this pipeline:\n",
        "\n",
        "| Notebook | Purpose | Output Folder |\n",
        "|-----------|----------|---------------|\n",
        "| **Synthetic Blood Pressure Generator** | Creates a 30-day synthetic blood pressure dataset | `data/synthetic_csv/` |\n",
        "| **Spanish Audio Generator** | Produces Spanish-language health questions from the dataset | `data/Spanish_audio/` |\n",
        "\n",
        "> 🔹 *Reviewers may also upload their own CSVs and audio recordings.*  \n",
        "> To compute **WER/CER/SER**, ensure a `ground_truth.csv` file exists with reference text for each input audio file.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚙️ Pipeline Overview\n",
        "\n",
        "| Step | Component | Description | Output |\n",
        "|------|------------|-------------|---------|\n",
        "| **1️⃣** | **ASR (Whisper)** | Transcribes Spanish medical audio into text | Spanish transcript |\n",
        "| **2️⃣** | **Translation (GPT)** | Translates Spanish → English for question understanding | `audio_translations.csv` |\n",
        "| **3️⃣** | **ASR Evaluation (Input)** | Compares transcriptions to ground truth → computes **WER**, **CER**, **SER** | `input_asr_metrics.csv` |\n",
        "| **4️⃣** | **LLM Analysis (GPT-4o-mini)** | Answers English questions using the blood pressure CSV context | `final_pipeline_results.csv` |\n",
        "| **5️⃣** | **Spanish TTS (gTTS)** | Converts English answers back into spoken Spanish | `tts_audio/*.wav` |\n",
        "| **6️⃣** | **ASR Evaluation (Output)** | Evaluates TTS intelligibility via Whisper ASR (WER/CER/SER) | `output_asr_metrics.csv` |\n",
        "| **7️⃣** | **Visualization** | Plots a bar chart comparing input vs. output ASR metrics | `asr_comparison_chart.png` |\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Translation Toggle\n",
        "\n",
        "This pipeline can use either **Whisper** or **GPT** for Spanish-to-English translation:\n",
        "\n",
        "| Mode | How to Enable | Description |\n",
        "|------|----------------|-------------|\n",
        "| 🧩 **GPT Translation (Recommended)** | `USE_GPT_TRANSLATION = True` | Uses GPT-4o-mini for domain-accurate translations (preserves medical terms and numbers). Requires OpenAI API key. |\n",
        "| ⚙️ **Whisper-Only Mode (Free)** | `USE_GPT_TRANSLATION = False` | Uses Whisper’s built-in `task=\"translate\"` mode. Works offline but may reduce translation accuracy. |\n",
        "\n",
        "You can change this toggle at the top of the ASR module.\n",
        "\n",
        "---\n",
        "\n",
        "### 📊 Evaluation Metrics\n",
        "\n",
        "| Metric | Description | Interpretation |\n",
        "|---------|-------------|----------------|\n",
        "| **WER (Word Error Rate)** | Measures overall transcription accuracy (substitutions, deletions, insertions) | Lower = better |\n",
        "| **CER (Character Error Rate)** | Captures fine-grained textual differences | Sensitive to short utterances |\n",
        "| **SER (Sentence Error Rate)** | Binary metric — whether a sentence matches exactly | 0 = perfect, 1 = any error |\n",
        "\n",
        "All metrics are computed for both **input** (Spanish audio → text) and **output** (Spanish TTS → text) stages.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧰 System Notes\n",
        "- Default Whisper model is `\"base\"` but can be changed to `\"small\"`, `\"medium\"`, or `\"large\"`.  \n",
        "- All directories (`data/`, `results/`, etc.) are auto-created by the pipeline.  \n",
        "- Results and charts are saved under `/results/` for easy retrieval.  \n",
        "- The notebook can run entirely on Google Colab; only GPT translation requires an OpenAI API key.\n",
        "\n",
        "---\n",
        "\n",
        "### 📂 Folder Structure\n",
        "\n",
        "```text\n",
        "/content/drive/MyDrive/HealthTequity-LLM/\n",
        "│\n",
        "├── data/\n",
        "│   ├── synthetic_csv/              ← Synthetic BP data + ground truth\n",
        "│   │   ├── synthetic_bp_one_person.csv\n",
        "│   │   └── ground_truth.csv\n",
        "│   │\n",
        "│   └── Spanish_audio/              ← Input Spanish audio files\n",
        "│       ├── question_1_es.wav\n",
        "│       ├── question_2_es.wav\n",
        "│       └── ...\n",
        "│\n",
        "├── results/\n",
        "│   ├── llm_outputs/                ← Translations + LLM responses\n",
        "│   ├── tts_audio/                  ← Spanish audio answers\n",
        "│   └── evaluation_metrics/         ← WER/CER/SER + chart\n",
        "│\n",
        "├── Generate_Spanish_Audio.ipynb\n",
        "├── Generate_Synthetic_BP_Dataset.ipynb\n",
        "└── HealthTequity_VoicePipeline.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IBCmfYIB76nO",
        "outputId": "299d9d4d-bc74-403f-ad35-b7cc5d29c190"
      },
      "id": "IBCmfYIB76nO",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/803.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m798.7/803.2 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton>=2->openai-whisper) (75.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.11.1.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper) (3.0.3)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=56c648be257569f415b4ed8a68960fb8656e920a8cd575111b13783d4f579cf4\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/d2/20/09ec9bef734d126cba375b15898010b6cc28578d8afdde5869\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: openai-whisper\n",
            "Successfully installed openai-whisper-20250625\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "whisper"
                ]
              },
              "id": "515fb7e7f9854060b8b5d5ddeff28ac4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0pkaokMr0f7Q",
      "metadata": {
        "id": "0pkaokMr0f7Q"
      },
      "source": [
        "## 📁 Step 1 — Mount Google Drive and Sync Project Repository\n",
        "\n",
        "This step connects your Colab environment to Google Drive and ensures you have the latest version of the **HealthTequity-LLM** project.\n",
        "\n",
        "**What this does:**\n",
        "1. Clears any previously mounted Drive (prevents the “mountpoint already contains files” error).\n",
        "2. Mounts Google Drive at `/content/drive`.\n",
        "3. Clones the `HealthTequity-LLM` GitHub repository into your Drive if it doesn’t exist,  \n",
        "   or updates it with the latest version if it already exists.\n",
        "\n",
        "**After this cell runs successfully:**\n",
        "- You’ll be working directly from  \n",
        "  `/content/drive/MyDrive/HealthTequity-LLM`\n",
        "- All project data, code, and outputs will stay persistent in your Google Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "R9GGDiUrHnS2",
      "metadata": {
        "id": "R9GGDiUrHnS2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d3e5ae-c5cb-4126-e28b-5884bce6fdc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚙️ Clearing existing mountpoint: /content/drive\n",
            "⚠️ Warning: Could not fully clear mountpoint: [Errno 2] No such file or directory: '/content/drive/.Trash-0/info'\n",
            "🔗 Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "🔄 Repository already exists — updating...\n",
            "/content/drive/MyDrive/HealthTequity-LLM\n",
            "fatal: not a git repository (or any parent up to mount point /content)\n",
            "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
            "fatal: not a git repository (or any parent up to mount point /content)\n",
            "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
            "/content/drive/MyDrive/HealthTequity-LLM\n",
            "✅ Environment ready. Working directory: /content/drive/MyDrive/HealthTequity-LLM\n"
          ]
        }
      ],
      "source": [
        "# ==========================================================\n",
        "# 📁 STEP 1 — Mount Google Drive and Clone/Update Repository\n",
        "# ==========================================================\n",
        "from google.colab import drive\n",
        "import os, shutil\n",
        "\n",
        "MOUNT_POINT = '/content/drive'\n",
        "REPO_URL = \"https://github.com/nattaran/HealthTequity-LLM.git\"\n",
        "REPO_PATH = f\"{MOUNT_POINT}/MyDrive/HealthTequity-LLM\"\n",
        "\n",
        "# --- Clean any existing mountpoint to prevent ValueError ---\n",
        "if os.path.exists(MOUNT_POINT) and os.path.isdir(MOUNT_POINT) and os.listdir(MOUNT_POINT):\n",
        "    print(f\"⚙️ Clearing existing mountpoint: {MOUNT_POINT}\")\n",
        "    try:\n",
        "        shutil.rmtree(MOUNT_POINT)\n",
        "        os.makedirs(MOUNT_POINT)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Warning: Could not fully clear mountpoint: {e}\")\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "print(\"🔗 Mounting Google Drive...\")\n",
        "drive.mount(MOUNT_POINT, force_remount=True)\n",
        "\n",
        "# --- Clone or update GitHub repo ---\n",
        "if not os.path.exists(REPO_PATH):\n",
        "    print(f\"📦 Cloning repository into {REPO_PATH}...\")\n",
        "    !git clone {REPO_URL} {REPO_PATH}\n",
        "else:\n",
        "    print(\"🔄 Repository already exists — updating...\")\n",
        "    %cd {REPO_PATH}\n",
        "    !git fetch origin\n",
        "    !git pull\n",
        "\n",
        "%cd {REPO_PATH}\n",
        "print(f\"✅ Environment ready. Working directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6wKfk2K51qpt",
      "metadata": {
        "id": "6wKfk2K51qpt"
      },
      "source": [
        "## ⚙️ Step 2 — Install Project Dependencies\n",
        "\n",
        "This step installs all the required Python packages for the HealthTequity-LLM pipeline.\n",
        "\n",
        "**Notes:**\n",
        "- Run this cell once per Colab session.\n",
        "- Dependencies are listed in `requirements.txt` at the repository root.\n",
        "- You can add or pin package versions there (e.g., `whisper==1.0`, `jiwer==3.0.2`).\n",
        "- Colab may show warnings for already-installed packages — you can safely ignore them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a53aba9f",
      "metadata": {
        "id": "a53aba9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4de016c5-3da1-491e-abcb-65baa61d2d7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "✅ Package installation complete.\n",
            "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "geopandas                                1.1.1\n",
            "jiwer                                    4.0.0\n",
            "matplotlib                               3.10.0\n",
            "matplotlib-inline                        0.1.7\n",
            "matplotlib-venn                          1.1.2\n",
            "openai                                   1.109.1\n",
            "openai-whisper                           20250625\n",
            "pandas                                   2.2.2\n",
            "pandas-datareader                        0.10.0\n",
            "pandas-gbq                               0.29.2\n",
            "pandas-stubs                             2.2.2.240909\n",
            "sklearn-pandas                           2.2.0\n",
            "torch                                    2.8.0+cu126\n",
            "torchao                                  0.10.0\n",
            "torchaudio                               2.8.0+cu126\n",
            "torchdata                                0.11.0\n",
            "torchsummary                             1.5.1\n",
            "torchtune                                0.6.1\n",
            "torchvision                              0.23.0+cu126\n",
            "whisper                                  1.1.10\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install project requirements (run once per session)\n",
        "# If you prefer to pin versions, ensure requirements.txt includes exact versions.\n",
        "!pip install -r requirements.txt\n",
        "# Verify installation of key modules\n",
        "import sys\n",
        "print(\"\\n✅ Package installation complete.\")\n",
        "print(\"Python version:\", sys.version)\n",
        "!pip list | grep -E \"openai|whisper|jiwer|pandas|torch|matplotlib\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3o7I6l8g-sXa",
      "metadata": {
        "id": "3o7I6l8g-sXa"
      },
      "source": [
        "## 📂 Step 3 — Project Paths and Data Dependencies\n",
        "\n",
        "This step defines the directory structure used by the **HealthTequity Voice Pipeline**  \n",
        "and ensures all necessary folders exist within your Google Drive.\n",
        "\n",
        "By default, the **Audio Generation Notebook** automatically produces:\n",
        "- Spanish question audio files (`.wav`) under `data/Spanish_audio/`\n",
        "- The corresponding `ground_truth.csv` file under `data/synthetic_csv/`\n",
        "- A synthetic blood-pressure dataset (`synthetic_bp_one_person.csv`) under the same folder\n",
        "\n",
        "These files are automatically detected when this pipeline runs.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧩 Reviewer Flexibility\n",
        "If you prefer to **test with your own data**, you can:\n",
        "- Upload custom `.wav` files to `data/Spanish_audio/`\n",
        "- Upload your own ground truth file to `data/synthetic_csv/ground_truth.csv`\n",
        "- Upload a new blood pressure dataset to `data/synthetic_csv/`\n",
        "\n",
        "Your files will be automatically used by the pipeline — no code modification needed.\n",
        "\n",
        "---\n",
        "\n",
        "### 🗂 Folder Overview\n",
        "\n",
        "| Folder | Purpose |\n",
        "|---------|----------|\n",
        "| `data/synthetic_csv/` | Synthetic or user-provided CSV datasets and ground truth |\n",
        "| `data/Spanish_audio/` | Input Spanish `.wav` question audio files |\n",
        "| `results/llm_outputs/` | LLM-generated question–answer CSV outputs |\n",
        "| `results/evaluation_metrics/` | ASR evaluation metrics (WER, CER, SER) |\n",
        "| `results/tts_audio/` | Generated Spanish audio (TTS) responses |\n",
        "\n",
        "All paths are created automatically under:\n",
        "`/content/drive/MyDrive/HealthTequity-LLM`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b5ec22c8",
      "metadata": {
        "id": "b5ec22c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bd70a85-1f03-4e54-9837-b7d999c3046b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Directory structure verified:\n",
            "📁 Project root:        /content/drive/MyDrive/HealthTequity-LLM\n",
            "📄 Ground truth file:   ✅ Found\n",
            "🎧 Spanish audio files: 12 found\n",
            "📊 Blood pressure CSV:  ⚠️ Missing\n",
            "📊 Results directory:   /content/drive/MyDrive/HealthTequity-LLM/results\n",
            "🧠 LLM outputs:         /content/drive/MyDrive/HealthTequity-LLM/results/llm_outputs\n",
            "🧮 Evaluation metrics:  /content/drive/MyDrive/HealthTequity-LLM/results/evaluation_metrics\n",
            "🔉 TTS audio:           /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio\n"
          ]
        }
      ],
      "source": [
        "# ==========================================================\n",
        "# 📂 STEP 3 — Project Paths and Data Dependencies\n",
        "# ==========================================================\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# --- Define project root in Google Drive ---\n",
        "PROJECT_ROOT = Path(\"/content/drive/MyDrive/HealthTequity-LLM\")\n",
        "\n",
        "# --- Define subdirectories ---\n",
        "DATA_DIR    = PROJECT_ROOT / \"data\"\n",
        "CSV_DIR     = DATA_DIR / \"synthetic_csv\"\n",
        "AUDIO_DIR   = DATA_DIR / \"Spanish_audio\"\n",
        "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
        "LLM_OUT     = RESULTS_DIR / \"llm_outputs\"\n",
        "EVAL_DIR    = RESULTS_DIR / \"evaluation_metrics\"\n",
        "TTS_DIR     = RESULTS_DIR / \"tts_audio\"\n",
        "\n",
        "# --- Create folders if missing (safe & repeatable) ---\n",
        "for p in [DATA_DIR, CSV_DIR, AUDIO_DIR, RESULTS_DIR, LLM_OUT, EVAL_DIR, TTS_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- Verify generated or uploaded data ---\n",
        "ground_truth = CSV_DIR / \"ground_truth.csv\"\n",
        "audio_files = list(AUDIO_DIR.glob(\"*.wav\"))\n",
        "bp_csv = CSV_DIR / \"synthetic_bp_one_person.csv\"\n",
        "\n",
        "print(\"✅ Directory structure verified:\")\n",
        "print(f\"📁 Project root:        {PROJECT_ROOT}\")\n",
        "print(f\"📄 Ground truth file:   {'✅ Found' if ground_truth.exists() else '⚠️ Missing'}\")\n",
        "print(f\"🎧 Spanish audio files: {len(audio_files)} found\")\n",
        "print(f\"📊 Blood pressure CSV:  {'✅ Found' if bp_csv.exists() else '⚠️ Missing'}\")\n",
        "print(f\"📊 Results directory:   {RESULTS_DIR}\")\n",
        "print(f\"🧠 LLM outputs:         {LLM_OUT}\")\n",
        "print(f\"🧮 Evaluation metrics:  {EVAL_DIR}\")\n",
        "print(f\"🔉 TTS audio:           {TTS_DIR}\")\n",
        "\n",
        "if not ground_truth.exists() or not audio_files:\n",
        "    print(\"\\n⚠️ Note: Run the Audio Generation Notebook or upload your own data to the above folders.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dwCedjfe_c-S",
      "metadata": {
        "id": "dwCedjfe_c-S"
      },
      "source": [
        "## 🔑 Step 4 — OpenAI API Key Initialization\n",
        "\n",
        "This step initializes the OpenAI client for all LLM operations (e.g., translations, question answering).\n",
        "\n",
        "**How it works:**\n",
        "- The code first looks for your API key in the Colab environment (`os.environ[\"OPENAI_API_KEY\"]`).\n",
        "- If no key is found, you’ll be securely prompted to paste it (input remains hidden).\n",
        "- Once entered, the key is stored in the current runtime session for later API calls.\n",
        "\n",
        "> 💡 **Security note:**  \n",
        "> Your key is **not saved permanently** — it will reset when the Colab runtime restarts.  \n",
        "> Reviewers can safely run this section with their own OpenAI API keys.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d2147149",
      "metadata": {
        "id": "d2147149",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ea9b7a7-90ac-48c2-be88-697c97ade5d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ OpenAI API key not found in environment.\n",
            "🔐 Paste your OpenAI API key (input hidden): ··········\n",
            "✅ OpenAI client initialized successfully.\n"
          ]
        }
      ],
      "source": [
        "# ==========================================================\n",
        "# 🔑 STEP 4 — OpenAI API Key Initialization\n",
        "# ==========================================================\n",
        "import os\n",
        "from getpass import getpass\n",
        "from openai import OpenAI\n",
        "\n",
        "# --- Check if API key exists; if not, prompt user securely ---\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    print(\"⚠️ OpenAI API key not found in environment.\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"🔐 Paste your OpenAI API key (input hidden): \").strip()\n",
        "else:\n",
        "    print(\"✅ Found existing OpenAI API key in environment.\")\n",
        "\n",
        "# --- Initialize client (raises error if key invalid) ---\n",
        "try:\n",
        "    client = OpenAI()\n",
        "    print(\"✅ OpenAI client initialized successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Failed to initialize OpenAI client: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y4vB58i5CTN_",
      "metadata": {
        "id": "Y4vB58i5CTN_"
      },
      "source": [
        "## 🗣️ Step 5 — ASR (Whisper) and Spanish → English Translation\n",
        "\n",
        "This section runs the **Automatic Speech Recognition (ASR)** and **translation** stage of the pipeline.  \n",
        "It is used on both sides of the workflow:\n",
        "\n",
        "1. **Input side** – transcribes Spanish question audio and translates it to English for LLM analysis.  \n",
        "2. **Output side** – re-transcribes generated Spanish TTS responses for ASR evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚙️ How It Works\n",
        "- **Whisper ASR** performs transcription directly from audio.  \n",
        "- **Whisper Translation Mode** (`task=\"translate\"`) produces English text automatically — no API key required.  \n",
        "- Optionally, reviewers can enable **OpenAI GPT translation** for more fluent medical phrasing.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 Configuration\n",
        "You can control translation behavior with this flag:\n",
        "```python\n",
        "USE_GPT_TRANSLATION = True  # Set True to use GPT-based translation instead of Whisper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0128eb68",
      "metadata": {
        "id": "0128eb68"
      },
      "outputs": [],
      "source": [
        "# ==========================================================\n",
        "# 🗣️ STEP 5 — ASR (Whisper) and Spanish → English Translation\n",
        "# ==========================================================\n",
        "import whisper\n",
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Toggle for optional GPT translation ---\n",
        "USE_GPT_TRANSLATION = False   # Default: Whisper only (free)\n",
        "WHISPER_MODEL_SIZE = \"tiny\"   # Adjustable model size (\"tiny\", \"small\", \"medium\", \"large\")\n",
        "\n",
        "def transcribe_spanish_audio(model, audio_path: Path):\n",
        "    \"\"\"\n",
        "    Transcribe a single Spanish audio file using Whisper.\n",
        "    Returns Spanish text and detected language.\n",
        "    \"\"\"\n",
        "    result = model.transcribe(str(audio_path), language=\"es\", task=\"transcribe\", verbose=False)\n",
        "    return result[\"text\"].strip(), result.get(\"language\", \"unknown\")\n",
        "\n",
        "def translate_audio_whisper(model, audio_path: Path):\n",
        "    \"\"\"\n",
        "    Use Whisper’s translation mode to directly translate Spanish → English.\n",
        "    \"\"\"\n",
        "    result = model.transcribe(str(audio_path), task=\"translate\", verbose=False)\n",
        "    return result[\"text\"].strip()\n",
        "\n",
        "def translate_spanish_to_english_gpt(spanish_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Optional GPT translation for higher fluency (requires OpenAI API key).\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"Translate the following Spanish medical transcription into clear, faithful English:\\n\\n\"\n",
        "        + spanish_text\n",
        "    )\n",
        "    result = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        temperature=0,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "    return result.choices[0].message.content.strip()\n",
        "\n",
        "def process_and_translate_audio(audio_folder: Path, output_csv: Path,\n",
        "                                model_size: str = WHISPER_MODEL_SIZE) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Run Whisper ASR on all .wav files in a folder,\n",
        "    translate Spanish → English (Whisper or GPT),\n",
        "    and save results to CSV.\n",
        "    \"\"\"\n",
        "    print(f\"🎧 Loading Whisper model: {model_size}\")\n",
        "    model = whisper.load_model(model_size)\n",
        "\n",
        "    audio_files = sorted([f for f in os.listdir(audio_folder) if f.endswith(\".wav\")])\n",
        "    if not audio_files:\n",
        "        print(f\"⚠️ No audio files found in {audio_folder}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    results = []\n",
        "    print(f\"🔍 Processing {len(audio_files)} audio files...\")\n",
        "\n",
        "    for fname in audio_files:\n",
        "        audio_path = audio_folder / fname\n",
        "        # 1️⃣ Spanish transcription\n",
        "        es_text, detected_lang = transcribe_spanish_audio(model, audio_path)\n",
        "        # 2️⃣ Translation (Whisper → English or GPT optional)\n",
        "        if USE_GPT_TRANSLATION:\n",
        "            en_text = translate_spanish_to_english_gpt(es_text)\n",
        "        else:\n",
        "            en_text = translate_audio_whisper(model, audio_path)\n",
        "\n",
        "        results.append({\n",
        "            \"audio_file\": fname,\n",
        "            \"spanish_transcription\": es_text,\n",
        "            \"english_translation\": en_text,\n",
        "            \"language_detected\": detected_lang\n",
        "        })\n",
        "        print(f\"✅ {fname} → processed\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"\\n💾 Results saved to: {output_csv}\")\n",
        "    return df\n",
        "\n",
        "# Example (do not auto-run):\n",
        "# trans_csv = LLM_OUT / \"audio_translations.csv\"\n",
        "# trans_df = process_and_translate_audio(AUDIO_DIR, trans_csv, model_size=\"base\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B0hE4n4YDQpV",
      "metadata": {
        "id": "B0hE4n4YDQpV"
      },
      "source": [
        "## 🧮 Step 6 — Unified ASR Evaluation (WER / CER / SER)\n",
        "\n",
        "This section defines a **single reusable evaluation module** for computing ASR performance metrics on any\n",
        "pair of *ground-truth* and *predicted* transcriptions.\n",
        "\n",
        "It is used in two contexts:\n",
        "1. **Input side evaluation** – compares Whisper’s transcribed Spanish questions against the ground-truth CSV.  \n",
        "2. **Output side evaluation** – compares Whisper’s re-transcription of TTS-generated Spanish responses\n",
        "   against the LLM-generated ground-truth Spanish text.\n",
        "\n",
        "---\n",
        "\n",
        "### 📏 Metrics Computed\n",
        "| Metric | Description |\n",
        "|---------|--------------|\n",
        "| **WER (Word Error Rate)** | Fraction of words incorrectly predicted (insertions + deletions + substitutions) / total words |\n",
        "| **CER (Character Error Rate)** | Character-level edit distance normalized by text length |\n",
        "| **SER (Sentence Error Rate)** | Percentage of sentences that are not identical to ground truth |\n",
        "\n",
        "---\n",
        "\n",
        "### ⚙️ How to Use\n",
        "Call the function below with:\n",
        "```python\n",
        "evaluate_asr_whisper(\n",
        "    gt_csv=Path(...),          # CSV with ground_truth_text\n",
        "    audio_folder=Path(...),    # folder of .wav files to evaluate\n",
        "    output_csv=Path(...),      # where to save evaluation results\n",
        "    model_size=\"base\"          # whisper model size (default)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gJMncazPoep-",
      "metadata": {
        "id": "gJMncazPoep-"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer Levenshtein"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1t7oUww8NAz",
        "outputId": "a79eb62b-beb3-4873-cab5-db5725b9bc1c"
      },
      "id": "-1t7oUww8NAz",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: Levenshtein in /usr/local/lib/python3.12/dist-packages (0.27.1)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.3.0)\n",
            "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.12/dist-packages (from jiwer) (3.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "24d16763",
      "metadata": {
        "id": "24d16763"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ==========================================================\n",
        "# 🧩 STEP 8 — UNIFIED ASR EVALUATION FUNCTION (FINAL)\n",
        "# ==========================================================\n",
        "import os\n",
        "import pandas as pd\n",
        "import whisper\n",
        "import re, unicodedata, Levenshtein\n",
        "from jiwer import process_words\n",
        "from pathlib import Path\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    \"\"\"Lowercase, strip accents, and remove punctuation.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', text)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def evaluate_asr_whisper(\n",
        "    gt_csv: Path,\n",
        "    audio_folder: Path,\n",
        "    output_csv: Path,\n",
        "    model_size: str = \"base\",\n",
        "    gt_text_col: str = \"ground_truth_text\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate ASR performance using Whisper and compute WER, CER, SER.\n",
        "    Works for both input and output sides of the pipeline.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    gt_csv : Path\n",
        "        CSV file containing at least columns [audio_file, ground_truth_text]\n",
        "    audio_folder : Path\n",
        "        Directory containing .wav audio files to evaluate\n",
        "    output_csv : Path\n",
        "        Output CSV for detailed ASR metrics\n",
        "    model_size : str, optional\n",
        "        Whisper model size to load (default: 'base')\n",
        "        Options: 'tiny', 'base', 'small', 'medium', 'large'\n",
        "    gt_text_col : str, optional\n",
        "        Column name for the reference text (default: 'ground_truth_text')\n",
        "    \"\"\"\n",
        "    if not Path(gt_csv).exists():\n",
        "        raise FileNotFoundError(f\"❌ Ground truth CSV not found at: {gt_csv}\")\n",
        "    if not Path(audio_folder).exists():\n",
        "        raise FileNotFoundError(f\"❌ Audio folder not found at: {audio_folder}\")\n",
        "\n",
        "    df_gt = pd.read_csv(gt_csv)\n",
        "    if \"audio_file\" not in df_gt.columns or gt_text_col not in df_gt.columns:\n",
        "        raise ValueError(f\"❌ CSV must contain ['audio_file', '{gt_text_col}'] columns.\")\n",
        "\n",
        "    print(f\"🎧 Loading Whisper model: {model_size}\")\n",
        "    model = whisper.load_model(model_size)\n",
        "\n",
        "    results = []\n",
        "    print(f\"🔍 Evaluating {len(df_gt)} audio files...\")\n",
        "\n",
        "    for _, row in df_gt.iterrows():\n",
        "        audio_name = row[\"audio_file\"]\n",
        "        gt_text = str(row[gt_text_col]).strip()\n",
        "        audio_path = Path(audio_folder) / audio_name\n",
        "        if not audio_path.exists():\n",
        "            print(f\"⚠️ Missing audio file: {audio_name}\")\n",
        "            continue\n",
        "\n",
        "        # --- Transcribe ---\n",
        "        result = model.transcribe(str(audio_path), language=\"es\", task=\"transcribe\", verbose=False)\n",
        "        hyp_text = result[\"text\"].strip()\n",
        "\n",
        "        # --- Normalize & compute metrics ---\n",
        "        gt_norm, hyp_norm = normalize_text(gt_text), normalize_text(hyp_text)\n",
        "        measures = process_words(gt_norm, hyp_norm)\n",
        "        wer = round(measures.wer, 4)\n",
        "        cer = round(Levenshtein.distance(gt_norm, hyp_norm) / max(len(gt_norm), 1), 4)\n",
        "        ser = 0 if gt_norm == hyp_norm else 1\n",
        "\n",
        "        results.append({\n",
        "            \"audio_file\": audio_name,\n",
        "            \"ground_truth\": gt_text,\n",
        "            \"whisper_transcription\": hyp_text,\n",
        "            \"WER\": wer,\n",
        "            \"CER\": cer,\n",
        "            \"SER\": ser,\n",
        "            \"Substitutions\": measures.substitutions,\n",
        "            \"Deletions\": measures.deletions,\n",
        "            \"Insertions\": measures.insertions\n",
        "        })\n",
        "        print(f\"✅ {audio_name} → WER={wer}, CER={cer}, SER={ser}\")\n",
        "\n",
        "    # --- Save results ---\n",
        "    out_df = pd.DataFrame(results)\n",
        "    out_df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"\\n💾 ASR metrics saved to: {output_csv}\")\n",
        "    print(f\"📊 Average WER={out_df['WER'].mean():.3f}, CER={out_df['CER'].mean():.3f}, SER={out_df['SER'].mean():.3f}\")\n",
        "    return out_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "U7dkMDTZIYQR",
      "metadata": {
        "id": "U7dkMDTZIYQR"
      },
      "source": [
        "## 📊 Step 7 — ASR Metrics Visualization (Input vs Output)\n",
        "\n",
        "This step visualizes the ASR performance of the pipeline for both the **input** and **output** stages.\n",
        "\n",
        "**Purpose:**\n",
        "- The **Input ASR** evaluates how well Whisper transcribes Spanish question audios compared to ground truth.  \n",
        "- The **Output ASR** evaluates Whisper’s accuracy in re-transcribing the TTS-generated Spanish answers.\n",
        "\n",
        "The plot shows average **WER (Word Error Rate)**, **CER (Character Error Rate)**, and **SER (Sentence Error Rate)** side-by-side.\n",
        "\n",
        "**Expected Input Files:**\n",
        "- `results/evaluation_metrics/input_asr_metrics.csv`\n",
        "- `results/evaluation_metrics/output_asr_metrics.csv`\n",
        "\n",
        "**Output:**\n",
        "- A bar chart saved as `results/evaluation_metrics/asr_comparison_chart.png`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "314K3oxCIaDu",
      "metadata": {
        "id": "314K3oxCIaDu"
      },
      "outputs": [],
      "source": [
        "# ==========================================================\n",
        "# 📊 STEP 7 — ASR Metrics Visualization (Input vs Output)\n",
        "# ==========================================================\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "def load_asr_summary(csv_path: Path):\n",
        "    \"\"\"Load ASR metrics and compute average WER, CER, SER.\"\"\"\n",
        "    if not csv_path.exists():\n",
        "        print(f\"⚠️ Missing file: {csv_path}\")\n",
        "        return {\"WER\": None, \"CER\": None, \"SER\": None}\n",
        "    df = pd.read_csv(csv_path)\n",
        "    return {\n",
        "        \"WER\": df[\"WER\"].mean(),\n",
        "        \"CER\": df[\"CER\"].mean(),\n",
        "        \"SER\": df[\"SER\"].mean()\n",
        "    }\n",
        "\n",
        "def plot_asr_comparison(\n",
        "    input_asr_csv: Path,\n",
        "    output_asr_csv: Path,\n",
        "    output_dir: Path\n",
        "):\n",
        "    \"\"\"\n",
        "    Compare average ASR metrics between input and output sides,\n",
        "    and save a side-by-side bar chart visualization.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_asr_csv : Path\n",
        "        CSV file containing input-side ASR metrics.\n",
        "    output_asr_csv : Path\n",
        "        CSV file containing output-side ASR metrics.\n",
        "    output_dir : Path\n",
        "        Directory where the visualization will be saved.\n",
        "    \"\"\"\n",
        "    # --- Load average metrics ---\n",
        "    input_metrics = load_asr_summary(input_asr_csv)\n",
        "    output_metrics = load_asr_summary(output_asr_csv)\n",
        "\n",
        "    # --- Prepare data for plotting ---\n",
        "    metrics = [\"WER\", \"CER\", \"SER\"]\n",
        "    input_vals = [input_metrics[m] for m in metrics]\n",
        "    output_vals = [output_metrics[m] for m in metrics]\n",
        "\n",
        "    # --- Plot chart ---\n",
        "    fig, ax = plt.subplots(figsize=(7, 5))\n",
        "    x = range(len(metrics))\n",
        "    width = 0.35\n",
        "\n",
        "    ax.bar([i - width/2 for i in x], input_vals, width, label=\"Input ASR\", alpha=0.8)\n",
        "    ax.bar([i + width/2 for i in x], output_vals, width, label=\"Output ASR\", alpha=0.8)\n",
        "\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(metrics)\n",
        "    ax.set_ylabel(\"Error Rate\")\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_title(\"ASR Performance Comparison — Input vs Output\")\n",
        "    ax.legend()\n",
        "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # --- Save chart ---\n",
        "    save_path = Path(output_dir) / \"asr_comparison_chart.png\"\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"📊 ASR comparison chart saved to: {save_path}\")\n",
        "    return save_path\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eu9pvHxqJjH2",
      "metadata": {
        "id": "eu9pvHxqJjH2"
      },
      "source": [
        "## 🧠 Step 8 — LLM Question Answering (Blood Pressure Analysis)\n",
        "\n",
        "This step defines the **LLM-based question-answering function** for the HealthTequity Voice Pipeline.  \n",
        "The model uses the translated English questions from the Spanish audio and answers them using the provided synthetic blood-pressure dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚙️ How It Works\n",
        "- The dataset is passed to the model as a **CSV text block** (context).  \n",
        "- Each question (translated from Spanish → English) is analyzed in context.  \n",
        "- The LLM returns a structured **JSON response** containing:\n",
        "  - `\"answer\"` — a natural-language English explanation.\n",
        "  - `\"computed_fields\"` — optional numeric values or summaries used in reasoning.\n",
        "\n",
        "---\n",
        "\n",
        "### 📚 Behavior Rules\n",
        "- The LLM can only use the dataset for factual answers.  \n",
        "- It can cite external information **only** when describing “normal” blood pressure ranges.  \n",
        "- All outputs follow a conversational, user-friendly tone.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧩 Usage Example\n",
        "```python\n",
        "csv_text = (CSV_DIR / \"synthetic_bp_one_person.csv\").read_text()\n",
        "q = \"What is my average blood pressure this week?\"\n",
        "response = ask_gpt(q, csv_text)\n",
        "print(response[\"answer\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uvo1FsVgJyk7",
      "metadata": {
        "id": "uvo1FsVgJyk7"
      },
      "source": [
        "{\n",
        "  \"answer\": \"Your average systolic pressure this week was 118 mm Hg and your diastolic pressure was 77 mm Hg.\",\n",
        "  \"computed_fields\": {\"systolic_avg\": 118, \"diastolic_avg\": 77}\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4477ecb",
      "metadata": {
        "id": "f4477ecb"
      },
      "source": [
        "\n",
        "## Step 3 – LLM Question Answering <a id=\"qa\"></a>\n",
        "This section queries an LLM with English questions derived from the ASR+translation step and provides answers based on a tabular blood-pressure dataset.\n",
        "\n",
        "**Inputs**: A CSV file with synthetic blood-pressure records.  \n",
        "**Outputs**: English answers and associated computed fields.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "85f89584",
      "metadata": {
        "id": "85f89584"
      },
      "outputs": [],
      "source": [
        "\n",
        "#  ==========================================================\n",
        "# 🧠 STEP 8 — LLM Question Answering (Blood Pressure Analysis)\n",
        "# ==========================================================\n",
        "import json\n",
        "\n",
        "def ask_gpt(question_en: str, csv_block: str) -> dict:\n",
        "    \"\"\"\n",
        "    Query the LLM with an English question and the CSV dataset context.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    question_en : str\n",
        "        English question derived from Spanish transcription.\n",
        "    csv_block : str\n",
        "        CSV content as a text block for in-context grounding.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Dictionary containing:\n",
        "        - \"answer\" : str — model's English response\n",
        "        - \"computed_fields\" : dict — optional numeric details\n",
        "    \"\"\"\n",
        "\n",
        "    system_prompt = \"\"\"\n",
        "You are a careful and detail-oriented data analyst.\n",
        "\n",
        "You are given a synthetic blood pressure dataset in CSV format. It contains readings for one individual over the last 30 consecutive days, with the following columns:\n",
        "\n",
        "- date\n",
        "- age\n",
        "- sex\n",
        "- systolic_mmHg\n",
        "- diastolic_mmHg\n",
        "\n",
        "Use only the data in the CSV to answer all questions, except when normal blood pressure ranges are requested — in those cases, you may use external references but must cite your source.\n",
        "\n",
        "---\n",
        "\n",
        "🧠 Interpretation Guidelines:\n",
        "\n",
        "- \"Today\" refers to the most recent date in the dataset.\n",
        "- \"Yesterday\" means the date before \"today\" in the dataset.\n",
        "- \"Last week\" or \"last month\" refer to 7- or 30-day windows before \"today\".\n",
        "- If a date or range is unavailable, clearly say so.\n",
        "- Use conversational date formats like “October 12” instead of numeric ones.\n",
        "\n",
        "---\n",
        "\n",
        "💬 Answer Style:\n",
        "- Use natural, conversational English.\n",
        "- Address the user as “you”.\n",
        "- Respond clearly and concisely.\n",
        "\n",
        "---\n",
        "\n",
        "✅ Response Format:\n",
        "Always return valid JSON with this structure:\n",
        "\n",
        "{\n",
        "  \"answer\": \"<English answer>\",\n",
        "  \"computed_fields\": { \"numeric values used\" }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "    user_prompt = (\n",
        "        f\"CSV Dataset:\\n{csv_block}\\n\\n\"\n",
        "        f\"Question:\\n{question_en}\\n\\n\"\n",
        "        \"Please analyze the data and respond strictly in valid JSON format as defined above.\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            temperature=0,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "                {\"role\": \"user\", \"content\": user_prompt.strip()},\n",
        "            ],\n",
        "        )\n",
        "        answer_text = response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ OpenAI API error: {e}\")\n",
        "        return {\"answer\": \"Error: failed to retrieve response.\", \"computed_fields\": {}}\n",
        "\n",
        "    # Safely handle JSON parsing\n",
        "    try:\n",
        "        result = json.loads(answer_text)\n",
        "        if not isinstance(result, dict):\n",
        "            raise ValueError(\"Invalid JSON structure.\")\n",
        "    except Exception:\n",
        "        result = {\"answer\": answer_text, \"computed_fields\": {}}\n",
        "\n",
        "    return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ma2u3TE7ML5w",
      "metadata": {
        "id": "ma2u3TE7ML5w"
      },
      "source": [
        "## 🔊 Step 9 — Spanish Translation & Text-to-Speech (Optimized)\n",
        "\n",
        "This module converts each English answer from the LLM into natural-sounding **Spanish audio**.  \n",
        "It provides two flexible layers:\n",
        "\n",
        "1. **Translation (English → Spanish)** — by default uses **OpenAI GPT-4o-mini**,  \n",
        "   but automatically falls back to `googletrans` if an API key is not available.\n",
        "2. **Text-to-Speech (Spanish → Audio)** — uses **gTTS + pydub** (free and Colab-friendly).\n",
        "\n",
        "All outputs are saved as `.wav` files in the `results/tts_audio/` directory.\n",
        "\n",
        "---\n",
        "\n",
        "### 📥 Input / 📤 Output\n",
        "\n",
        "| Step | Input | Output |\n",
        "|------|--------|---------|\n",
        "| Translation | English text | Spanish text |\n",
        "| TTS | Spanish text | Spoken Spanish `.wav` file |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p4RdoHuOlzBf",
      "metadata": {
        "id": "p4RdoHuOlzBf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f48ecab5",
      "metadata": {
        "id": "f48ecab5"
      },
      "outputs": [],
      "source": [
        "# ==========================================================\n",
        "# 🔊 STEP 9 — SPANISH TRANSLATION & TEXT-TO-SPEECH (OPTIMIZED)\n",
        "# ==========================================================\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def translate_to_spanish(text_en: str) -> str:\n",
        "    \"\"\"\n",
        "    Translate English text to Spanish.\n",
        "    Uses OpenAI GPT if available, otherwise falls back to googletrans.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text_en : str\n",
        "        English text to translate.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Spanish translation.\n",
        "    \"\"\"\n",
        "    # --- 1️⃣ Try OpenAI translation first ---\n",
        "    try:\n",
        "        if \"client\" in globals():\n",
        "            prompt = (\n",
        "                \"Translate the following English medical answer into clear, \"\n",
        "                \"natural Spanish:\\n\\n\" + text_en\n",
        "            )\n",
        "            resp = client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                temperature=0,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            )\n",
        "            return resp.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ OpenAI translation failed: {e}\")\n",
        "\n",
        "    # --- 2️⃣ Fallback: googletrans (free) ---\n",
        "    try:\n",
        "        from googletrans import Translator\n",
        "        translator = Translator()\n",
        "        return translator.translate(text_en, src=\"en\", dest=\"es\").text\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ googletrans fallback failed: {e}\")\n",
        "        return \"(translation unavailable)\"\n",
        "\n",
        "def text_to_speech_spanish(text_es: str, out_wav_path: Path):\n",
        "    \"\"\"\n",
        "    Generate Spanish TTS audio (free fallback using gTTS + pydub).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text_es : str\n",
        "        Spanish text to synthesize.\n",
        "    out_wav_path : Path\n",
        "        Destination path for the WAV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from gtts import gTTS\n",
        "        from pydub import AudioSegment\n",
        "\n",
        "        out_wav_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        tmp_mp3 = out_wav_path.with_suffix(\".mp3\")\n",
        "\n",
        "        # Generate MP3 and convert to WAV\n",
        "        gTTS(text_es, lang=\"es\").save(tmp_mp3)\n",
        "        AudioSegment.from_mp3(tmp_mp3).export(out_wav_path, format=\"wav\")\n",
        "        os.remove(tmp_mp3)\n",
        "\n",
        "        print(f\"✅ Spanish TTS saved → {out_wav_path.name}\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"TTS generation failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gTTS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "JKk_qJ6T9rtr",
        "outputId": "52973efc-eeed-4686-d6da-180d988c1d04"
      },
      "id": "JKk_qJ6T9rtr",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gTTS\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from gTTS) (2.32.4)\n",
            "Collecting click<8.2,>=7.1 (from gTTS)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->gTTS) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->gTTS) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->gTTS) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->gTTS) (2025.10.5)\n",
            "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: click, gTTS\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.3.0\n",
            "    Uninstalling click-8.3.0:\n",
            "      Successfully uninstalled click-8.3.0\n",
            "Successfully installed click-8.1.8 gTTS-2.5.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "click"
                ]
              },
              "id": "9d6acac833f4405caa2e1d7df8d54c4a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CszJZQ6ZOPbk",
      "metadata": {
        "id": "CszJZQ6ZOPbk"
      },
      "source": [
        "## 🚀 Step 10 — Full Pipeline Integration\n",
        "\n",
        "This step executes the **complete HealthTequity Voice Pipeline**, combining all previous components into a single, reproducible workflow.\n",
        "\n",
        "The pipeline processes spoken Spanish questions, interprets them through an AI-driven analytics system, and produces accurate Spanish spoken answers grounded in tabular blood-pressure data.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧩 Workflow Overview\n",
        "\n",
        "| Step | Process | Description | Output |\n",
        "|------|----------|--------------|---------|\n",
        "| 1️⃣ | **Input ASR + Translation** | Transcribes each Spanish question and translates it into English. | `audio_translations.csv` |\n",
        "| 2️⃣ | **Input ASR Evaluation** | Compares Whisper transcriptions with ground-truth text to compute WER, CER, and SER. | `input_asr_metrics.csv` |\n",
        "| 3️⃣ | **CSV Grounding** | Loads the synthetic blood-pressure dataset as the context for LLM reasoning. | In-memory CSV string |\n",
        "| 4️⃣ | **LLM Question Answering** | Uses GPT to analyze the dataset and answer each English question. | English answer text |\n",
        "| 5️⃣ | **Spanish Translation + TTS** | Converts each English answer into natural Spanish and generates speech audio. | `results/tts_audio/*.wav` |\n",
        "| 6️⃣ | **Output ASR Evaluation** | Transcribes TTS-generated Spanish audio and compares it to ground-truth Spanish answers. | `output_asr_metrics.csv` |\n",
        "| 7️⃣ | **Visualization** | Displays a side-by-side comparison of WER, CER, and SER for input vs. output ASR. | Matplotlib chart |\n",
        "\n",
        "---\n",
        "\n",
        "### 📁 Input Requirements\n",
        "- **`data/synthetic_csv/synthetic_bp_one_person.csv`** — Blood-pressure dataset  \n",
        "- **`data/Spanish_audio/`** — Spanish question audio files  \n",
        "- **`data/synthetic_csv/ground_truth.csv`** — Ground-truth transcription for ASR evaluation  \n",
        "\n",
        "Reviewers may replace these files with their own data to test other datasets or audio samples.\n",
        "\n",
        "---\n",
        "\n",
        "### 💾 Output Artifacts\n",
        "All generated files are automatically stored in the following directories:\n",
        "- **Transcriptions & LLM results:** `results/llm_outputs/`\n",
        "- **Evaluation metrics:** `results/evaluation_metrics/`\n",
        "- **Spanish speech audio:** `results/tts_audio/`\n",
        "\n",
        "---\n",
        "\n",
        "### ⚙️ Configuration\n",
        "You can select different Whisper models for accuracy/speed trade-offs using the parameter:\n",
        "```python\n",
        "whisper_model_size=\"base\"  # options: \"tiny\", \"base\", \"small\", \"medium\", \"large\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "852067e6",
      "metadata": {
        "id": "852067e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf861abf-a80f-4aca-e7e4-ed6ea101a4bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "🎙️ STEP 1: Input ASR + Translation (Spanish → English)\n",
            "==============================\n",
            "🎧 Loading Whisper model: base\n",
            "🔍 Processing 12 audio files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 516/516 [00:00<00:00, 2519.99frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: Spanish\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 516/516 [00:00<00:00, 2704.37frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q10_es.wav → processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 628/628 [00:00<00:00, 2815.84frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: Spanish\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 628/628 [00:00<00:00, 3901.22frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q11_es.wav → processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 669/669 [00:00<00:00, 2616.79frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: Spanish\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 669/669 [00:00<00:00, 3265.83frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q12_es.wav → processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 458/458 [00:00<00:00, 2216.77frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: Spanish\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 458/458 [00:00<00:00, 505.79frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q1_es.wav → processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 477/477 [00:00<00:00, 2553.11frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: Spanish\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 477/477 [00:00<00:00, 594.34frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q2_es.wav → processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 427/427 [00:00<00:00, 1895.11frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: Spanish\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 427/427 [00:00<00:00, 2502.93frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q3_es.wav → processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 573/573 [00:00<00:00, 1850.20frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: Spanish\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 573/573 [00:00<00:00, 2552.76frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q4_es.wav → processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 328/328 [00:00<00:00, 1609.54frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: Spanish\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 328/328 [00:00<00:00, 2009.58frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q5_es.wav → processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 405/405 [00:00<00:00, 1332.65frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: Spanish\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 405/405 [00:00<00:00, 534.76frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q6_es.wav → processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 448/448 [00:00<00:00, 2309.86frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: Spanish\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 448/448 [00:00<00:00, 783.92frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q7_es.wav → processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 607/607 [00:00<00:00, 3191.59frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: Spanish\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 607/607 [00:00<00:00, 4156.37frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q8_es.wav → processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 739/739 [00:00<00:00, 3298.16frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: Spanish\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 739/739 [00:00<00:00, 3803.65frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q9_es.wav → processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "💾 Results saved to: /content/drive/MyDrive/HealthTequity-LLM/results/llm_outputs/audio_translations.csv\n",
            "\n",
            "==============================\n",
            "📊 STEP 2: Evaluate Input ASR (WER / CER / SER)\n",
            "==============================\n",
            "🎧 Loading Whisper model: base\n",
            "🔍 Evaluating 12 audio files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 458/458 [00:00<00:00, 2246.82frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q1_es.wav → WER=0.1111, CER=0.0161, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 477/477 [00:00<00:00, 2585.47frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q2_es.wav → WER=0.0, CER=0.0, SER=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 427/427 [00:00<00:00, 2434.70frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q3_es.wav → WER=0.0, CER=0.0, SER=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 573/573 [00:00<00:00, 2589.96frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q4_es.wav → WER=0.0714, CER=0.0128, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 328/328 [00:00<00:00, 1955.14frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q5_es.wav → WER=0.0, CER=0.0, SER=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 405/405 [00:00<00:00, 2052.68frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q6_es.wav → WER=0.1, CER=0.0182, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 448/448 [00:00<00:00, 2334.20frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q7_es.wav → WER=0.0, CER=0.0, SER=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 607/607 [00:00<00:00, 3183.14frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q8_es.wav → WER=0.0, CER=0.0, SER=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 739/739 [00:00<00:00, 3522.61frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q9_es.wav → WER=0.0, CER=0.0, SER=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 516/516 [00:00<00:00, 2660.82frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q10_es.wav → WER=0.0, CER=0.0, SER=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 628/628 [00:00<00:00, 2759.99frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q11_es.wav → WER=0.0833, CER=0.0128, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 669/669 [00:00<00:00, 2692.14frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q12_es.wav → WER=0.0667, CER=0.0123, SER=1\n",
            "\n",
            "💾 ASR metrics saved to: /content/drive/MyDrive/HealthTequity-LLM/results/evaluation_metrics/input_asr_metrics.csv\n",
            "📊 Average WER=0.036, CER=0.006, SER=0.417\n",
            "\n",
            "==============================\n",
            "📈 STEP 3: LLM Question Answering\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n",
            "  m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n",
            "  m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n",
            "  elif re.match('(flt)p?( \\(default\\))?$', token):\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n",
            "  elif re.match('(dbl)p?( \\(default\\))?$', token):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Spanish TTS saved → answer_1_es.wav\n",
            "✅ Spanish TTS saved → answer_2_es.wav\n",
            "✅ Spanish TTS saved → answer_3_es.wav\n",
            "✅ Spanish TTS saved → answer_4_es.wav\n",
            "✅ Spanish TTS saved → answer_5_es.wav\n",
            "✅ Spanish TTS saved → answer_6_es.wav\n",
            "✅ Spanish TTS saved → answer_7_es.wav\n",
            "✅ Spanish TTS saved → answer_8_es.wav\n",
            "✅ Spanish TTS saved → answer_9_es.wav\n",
            "✅ Spanish TTS saved → answer_10_es.wav\n",
            "✅ Spanish TTS saved → answer_11_es.wav\n",
            "✅ Spanish TTS saved → answer_12_es.wav\n",
            "✅ Saved final results to: /content/drive/MyDrive/HealthTequity-LLM/results/llm_outputs/final_pipeline_results.csv\n",
            "\n",
            "==============================\n",
            "🧠 STEP 4: Evaluate Output ASR (WER / CER / SER)\n",
            "==============================\n",
            "🎧 Loading Whisper model: base\n",
            "🔍 Evaluating 12 audio files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1533/1533 [00:00<00:00, 3060.78frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_1_es.wav → WER=0.1111, CER=0.1863, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2275/2275 [00:03<00:00, 729.60frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_2_es.wav → WER=1.8571, CER=0.789, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4137/4137 [00:01<00:00, 2819.36frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_3_es.wav → WER=0.8529, CER=0.512, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4862/4862 [00:01<00:00, 3296.47frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_4_es.wav → WER=0.1923, CER=0.0734, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 940/940 [00:00<00:00, 3417.72frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_5_es.wav → WER=0.0, CER=0.0, SER=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3508/3508 [00:01<00:00, 3307.39frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_6_es.wav → WER=0.0588, CER=0.0164, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5940/5940 [00:02<00:00, 2706.05frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_7_es.wav → WER=0.1818, CER=0.1758, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4070/4070 [00:01<00:00, 3379.82frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_8_es.wav → WER=0.2923, CER=0.2438, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2553/2553 [00:02<00:00, 893.52frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_9_es.wav → WER=0.2143, CER=0.2667, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2544/2544 [00:01<00:00, 2540.04frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_10_es.wav → WER=0.5926, CER=0.3691, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2608/2608 [00:01<00:00, 2245.49frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_11_es.wav → WER=0.0952, CER=0.0758, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7540/7540 [00:01<00:00, 4158.22frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_12_es.wav → WER=0.3261, CER=0.1569, SER=1\n",
            "\n",
            "💾 ASR metrics saved to: /content/drive/MyDrive/HealthTequity-LLM/results/evaluation_metrics/output_asr_metrics.csv\n",
            "📊 Average WER=0.398, CER=0.239, SER=0.917\n",
            "\n",
            "==============================\n",
            "📊 STEP 5: Visualize ASR Comparison\n",
            "==============================\n",
            "📊 ASR comparison chart saved to: /content/drive/MyDrive/HealthTequity-LLM/results/evaluation_metrics/asr_comparison_chart.png\n",
            "\n",
            "✅ Full pipeline completed successfully.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def run_full_pipeline(csv_path: Path, audio_folder: Path, whisper_model_size: str = \"base\"):\n",
        "    \"\"\"\n",
        "    Execute the full HealthTequity voice analysis pipeline.\n",
        "    Includes:\n",
        "      1️⃣ Input-side ASR & translation (Spanish → English)\n",
        "      2️⃣ ASR evaluation (WER / CER / SER)\n",
        "      3️⃣ LLM-driven Q&A on the blood pressure dataset\n",
        "      4️⃣ Spanish translation + TTS output\n",
        "      5️⃣ Output-side ASR evaluation (WER / CER / SER)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    csv_path : Path\n",
        "        Path to the synthetic blood-pressure CSV.\n",
        "    audio_folder : Path\n",
        "        Directory containing input Spanish .wav files.\n",
        "    whisper_model_size : str, optional\n",
        "        Whisper model size (default: \"base\").\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Summary dictionary with key file paths for inspection.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"🎙️ STEP 1: Input ASR + Translation (Spanish → English)\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    trans_csv = LLM_OUT / \"audio_translations.csv\"\n",
        "    _ = process_and_translate_audio(audio_folder, trans_csv, model_size=whisper_model_size)\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"📊 STEP 2: Evaluate Input ASR (WER / CER / SER)\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    gt_csv = CSV_DIR / \"ground_truth.csv\"\n",
        "    asr_csv = EVAL_DIR / \"input_asr_metrics.csv\"\n",
        "    _ = evaluate_asr_whisper(\n",
        "        gt_csv=gt_csv,\n",
        "        audio_folder=audio_folder,\n",
        "        output_csv=asr_csv,\n",
        "        model_size=whisper_model_size,\n",
        "        gt_text_col=\"ground_truth\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"📈 STEP 3: LLM Question Answering\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    df_bp = pd.read_csv(csv_path)\n",
        "    csv_block = df_bp.to_csv(index=False)\n",
        "    trans_df = pd.read_csv(trans_csv)\n",
        "\n",
        "    results = []\n",
        "    for i, row in trans_df.iterrows():\n",
        "        q_en = row[\"english_translation\"]\n",
        "        ans = ask_gpt(q_en, csv_block)\n",
        "        ans_en = ans.get(\"answer\", \"\").strip()\n",
        "        ans_es = translate_to_spanish(ans_en)\n",
        "\n",
        "        # Generate Spanish TTS output\n",
        "        out_wav = TTS_DIR / f\"answer_{i + 1}_es.wav\"\n",
        "        text_to_speech_spanish(ans_es, out_wav)\n",
        "\n",
        "        results.append({\n",
        "            \"question_number\": i + 1,\n",
        "            \"audio_file_in\": row[\"audio_file\"],\n",
        "            \"spanish_question\": row[\"spanish_transcription\"],\n",
        "            \"english_question\": q_en,\n",
        "            \"english_answer\": ans_en,\n",
        "            \"spanish_answer\": ans_es,\n",
        "            \"audio_file\": str(out_wav),  # ✅ unified column for ASR evaluation\n",
        "            \"computed_fields\": json.dumps(ans.get(\"computed_fields\", {}))\n",
        "        })\n",
        "\n",
        "    final_csv = LLM_OUT / \"final_pipeline_results.csv\"\n",
        "    pd.DataFrame(results).to_csv(final_csv, index=False)\n",
        "    print(f\"✅ Saved final results to: {final_csv}\")\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"🧠 STEP 4: Evaluate Output ASR (WER / CER / SER)\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    output_asr_csv = EVAL_DIR / \"output_asr_metrics.csv\"\n",
        "    _ = evaluate_asr_whisper(\n",
        "        gt_csv=final_csv,\n",
        "        audio_folder=TTS_DIR,\n",
        "        output_csv=output_asr_csv,\n",
        "        model_size=whisper_model_size,\n",
        "        gt_text_col=\"spanish_answer\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"📊 STEP 5: Visualize ASR Comparison\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    plot_asr_comparison(\n",
        "        input_asr_csv=asr_csv,\n",
        "        output_asr_csv=output_asr_csv,\n",
        "        output_dir=EVAL_DIR\n",
        "    )\n",
        "\n",
        "    print(\"\\n✅ Full pipeline completed successfully.\")\n",
        "    return {\n",
        "        \"transcriptions_csv\": str(trans_csv),\n",
        "        \"input_asr_metrics_csv\": str(asr_csv),\n",
        "        \"final_pipeline_csv\": str(final_csv),\n",
        "        \"output_asr_metrics_csv\": str(output_asr_csv)\n",
        "    }\n",
        "\n",
        "\n",
        "# Example (not auto-run in submission)\n",
        "bp_csv = CSV_DIR / \"synthetic_bp_77_male.csv\"\n",
        "_ = run_full_pipeline(bp_csv, AUDIO_DIR, whisper_model_size=\"base\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}