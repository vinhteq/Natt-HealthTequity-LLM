{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nattaran/HealthTequity-LLM/blob/main/HealthTequity_VoicePipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f46ec9ac",
      "metadata": {
        "id": "f46ec9ac"
      },
      "source": [
        "\n",
        "# HealthTequity Voice Pipeline\n",
        "\n",
        "## Introduction <a id=\"introduction\"></a>\n",
        "This notebook presents a modular voice pipeline for bilingual (Spanish‚ÄìEnglish) processing and evaluation. The system performs the following sequence:\n",
        "1) Automatic Speech Recognition (ASR) on Spanish audio inputs (Whisper base).  \n",
        "2) Machine translation (Spanish ‚Üí English) for downstream analysis.  \n",
        "3) Question answering over a tabular blood-pressure dataset using an LLM (GPT-40).  \n",
        "4) Back-translation to Spanish followed by text-to-speech (TTS).  \n",
        "5) Re-transcription of the generated Spanish audio using ASR and evaluation (WER, CER, SER).\n",
        "\n",
        "All steps are clearly separated, reproducible, and designed to be executed independently.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9ecd8b4",
      "metadata": {
        "id": "b9ecd8b4"
      },
      "source": [
        "\n",
        "## Table of Contents\n",
        "1. [Introduction](#introduction)  \n",
        "2. [Environment Setup](#setup)  \n",
        "3. [Folder Configuration](#paths)  \n",
        "4. [OpenAI Key Initialization](#openai)  \n",
        "5. [Step 1 ‚Äì ASR and Translation](#asr-translation)  \n",
        "6. [Step 2 ‚Äì ASR Evaluation](#asr-eval)  \n",
        "7. [Step 3 ‚Äì LLM Question Answering](#qa)  \n",
        "8. [Step 4 ‚Äì Translation and TTS](#tts)  \n",
        "9. [Step 5 ‚Äì Whisper Evaluation of TTS](#tts-eval)  \n",
        "10. [Step 6 ‚Äì Results Summary](#summary)  \n",
        "11. [Appendix](#appendix)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the mountpoint\n",
        "mountpoint = '/content/drive'\n",
        "\n",
        "# Check if the mountpoint exists and is not empty, then clear it\n",
        "if os.path.exists(mountpoint) and os.path.isdir(mountpoint) and os.listdir(mountpoint):\n",
        "    print(f\"Clearing mountpoint: {mountpoint}\")\n",
        "    try:\n",
        "        # Use shutil.rmtree to remove the directory and its contents\n",
        "        shutil.rmtree(mountpoint)\n",
        "        # Recreate the directory after removal\n",
        "        os.makedirs(mountpoint)\n",
        "    except Exception as e:\n",
        "        print(f\"Error clearing mountpoint: {e}\")\n",
        "        # If clearing fails, you might want to handle this differently,\n",
        "        # perhaps by raising an error or trying a different mountpoint.\n",
        "        # For now, we'll print an error and proceed, which might lead to\n",
        "        # the same ValueError again, but shows the attempt to clear.\n",
        "        pass\n",
        "\n",
        "\n",
        "drive.mount(mountpoint, force_remount=True)\n",
        "\n",
        "# Clone or update repo inside Drive\n",
        "repo_url = \"https://github.com/nattaran/HealthTequity-LLM.git\"\n",
        "repo_path = \"/content/drive/MyDrive/HealthTequity-LLM\"\n",
        "\n",
        "\n",
        "if not os.path.exists(repo_path):\n",
        "    !git clone {repo_url} {repo_path}\n",
        "else:\n",
        "    %cd {repo_path}\n",
        "    !git fetch origin\n",
        "    !git pull\n",
        "\n",
        "%cd {repo_path}\n",
        "print(\"‚úÖ Environment ready. Working from:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9GGDiUrHnS2",
        "outputId": "be0c19a5-3106-4765-b04d-1f174c0a281b"
      },
      "id": "R9GGDiUrHnS2",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clearing mountpoint: /content/drive\n",
            "Error clearing mountpoint: [Errno 125] Operation canceled: '/content/drive/.Encrypted/MyDrive'\n",
            "Mounted at /content/drive\n",
            "Cloning into '/content/drive/MyDrive/HealthTequity-LLM'...\n",
            "remote: Enumerating objects: 119, done.\u001b[K\n",
            "remote: Counting objects: 100% (119/119), done.\u001b[K\n",
            "remote: Compressing objects: 100% (114/114), done.\u001b[K\n",
            "remote: Total 119 (delta 51), reused 36 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (119/119), 1.99 MiB | 6.71 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n",
            "/content/drive/MyDrive/HealthTequity-LLM\n",
            "‚úÖ Environment ready. Working from: /content/drive/MyDrive/HealthTequity-LLM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23d83703",
      "metadata": {
        "id": "23d83703"
      },
      "source": [
        "\n",
        "## Environment Setup <a id=\"setup\"></a>\n",
        "Install dependencies from the provided `requirements.txt`. Run this cell once per runtime.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a53aba9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a53aba9f",
        "outputId": "f1ff5a32-9b75-4d1d-82c3-7ccf4da2e3c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git (from -r requirements.txt (line 22))\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-z8i0qxbb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-z8i0qxbb\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (2.2.2)\n",
            "Requirement already satisfied: matplotlib>=3.8.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (3.10.0)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (2.8.0+cu126)\n",
            "Requirement already satisfied: openai>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 15)) (1.109.1)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 16)) (0.12.0)\n",
            "Requirement already satisfied: whisper in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 20)) (1.1.10)\n",
            "Requirement already satisfied: ffmpeg-python>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 25)) (0.2.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 26)) (0.13.1)\n",
            "Requirement already satisfied: librosa>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 27)) (0.11.0)\n",
            "Requirement already satisfied: pydub>=0.25.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 28)) (0.25.1)\n",
            "Requirement already satisfied: jiwer>=3.0.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 31)) (4.0.0)\n",
            "Requirement already satisfied: python-Levenshtein>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 32)) (0.27.1)\n",
            "Requirement already satisfied: gtts>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 35)) (2.5.4)\n",
            "Requirement already satisfied: deep-translator>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 36)) (1.11.4)\n",
            "Requirement already satisfied: python-dotenv>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 39)) (1.1.1)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 40)) (4.67.1)\n",
            "Requirement already satisfied: vosk==0.3.45 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 43)) (0.3.45)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from vosk==0.3.45->-r requirements.txt (line 43)) (2.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vosk==0.3.45->-r requirements.txt (line 43)) (2.32.4)\n",
            "Requirement already satisfied: srt in /usr/local/lib/python3.12/dist-packages (from vosk==0.3.45->-r requirements.txt (line 43)) (3.5.3)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.12/dist-packages (from vosk==0.3.45->-r requirements.txt (line 43)) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->-r requirements.txt (line 7)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->-r requirements.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->-r requirements.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (3.2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (3.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (1.3.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.6.0->-r requirements.txt (line 16)) (2024.11.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from whisper->-r requirements.txt (line 20)) (1.17.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625->-r requirements.txt (line 22)) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625->-r requirements.txt (line 22)) (0.60.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from ffmpeg-python>=0.2.0->-r requirements.txt (line 25)) (1.0.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.16.2)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.0.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.1.2)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer>=3.0.3->-r requirements.txt (line 31)) (8.1.8)\n",
            "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.12/dist-packages (from jiwer>=3.0.3->-r requirements.txt (line 31)) (3.14.1)\n",
            "Requirement already satisfied: Levenshtein==0.27.1 in /usr/local/lib/python3.12/dist-packages (from python-Levenshtein>=0.25.0->-r requirements.txt (line 32)) (0.27.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.12/dist-packages (from deep-translator>=1.11.4->-r requirements.txt (line 36)) (4.13.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai>=1.12.0->-r requirements.txt (line 15)) (3.11)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator>=1.11.4->-r requirements.txt (line 36)) (2.8)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->vosk==0.3.45->-r requirements.txt (line 43)) (2.23)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=1.12.0->-r requirements.txt (line 15)) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=1.12.0->-r requirements.txt (line 15)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.12.0->-r requirements.txt (line 15)) (0.16.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper==20250625->-r requirements.txt (line 22)) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa>=0.10.1->-r requirements.txt (line 27)) (4.5.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.12.0->-r requirements.txt (line 15)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.12.0->-r requirements.txt (line 15)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.12.0->-r requirements.txt (line 15)) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vosk==0.3.45->-r requirements.txt (line 43)) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vosk==0.3.45->-r requirements.txt (line 43)) (2.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa>=0.10.1->-r requirements.txt (line 27)) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.0->-r requirements.txt (line 11)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.0->-r requirements.txt (line 11)) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install project requirements (run once per session)\n",
        "# If you prefer to pin versions, ensure requirements.txt includes exact versions.\n",
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2af3db58",
      "metadata": {
        "id": "2af3db58"
      },
      "source": [
        "\n",
        "## Folder Configuration <a id=\"paths\"></a>\n",
        "Centralized path configuration for data and results. This version assumes a Drive-based working directory that persists across sessions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b5ec22c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5ec22c8",
        "outputId": "3e1b4a83-c613-40c0-b273-5aa32839bd41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project root: /content/drive/MyDrive/HealthTequity-LLM\n",
            "Data dir: /content/drive/MyDrive/HealthTequity-LLM/data\n",
            "CSV dir: /content/drive/MyDrive/HealthTequity-LLM/data/synthetic_csv\n",
            "Audio dir: /content/drive/MyDrive/HealthTequity-LLM/data/Spanish_audio\n",
            "Results dir: /content/drive/MyDrive/HealthTequity-LLM/results\n",
            "LLM outputs: /content/drive/MyDrive/HealthTequity-LLM/results/llm_outputs\n",
            "Evaluation dir: /content/drive/MyDrive/HealthTequity-LLM/results/evaluation_metrics\n",
            "TTS dir: /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Project root in Google Drive (adjust if needed)\n",
        "PROJECT_ROOT = Path(\"/content/drive/MyDrive/HealthTequity-LLM\")\n",
        "\n",
        "# Data and results subfolders\n",
        "DATA_DIR     = PROJECT_ROOT / \"data\"\n",
        "CSV_DIR      = DATA_DIR / \"synthetic_csv\"\n",
        "AUDIO_DIR    = DATA_DIR / \"Spanish_audio\"\n",
        "RESULTS_DIR  = PROJECT_ROOT / \"results\"\n",
        "LLM_OUT      = RESULTS_DIR / \"llm_outputs\"\n",
        "EVAL_DIR     = RESULTS_DIR / \"evaluation_metrics\"\n",
        "TTS_DIR      = RESULTS_DIR / \"tts_audio\"\n",
        "\n",
        "# Create outputs if missing (idempotent)\n",
        "for p in [RESULTS_DIR, LLM_OUT, EVAL_DIR, TTS_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Project root:\", PROJECT_ROOT)\n",
        "print(\"Data dir:\", DATA_DIR)\n",
        "print(\"CSV dir:\", CSV_DIR)\n",
        "print(\"Audio dir:\", AUDIO_DIR)\n",
        "print(\"Results dir:\", RESULTS_DIR)\n",
        "print(\"LLM outputs:\", LLM_OUT)\n",
        "print(\"Evaluation dir:\", EVAL_DIR)\n",
        "print(\"TTS dir:\", TTS_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daf53a53",
      "metadata": {
        "id": "daf53a53"
      },
      "source": [
        "\n",
        "## OpenAI Key Initialization <a id=\"openai\"></a>\n",
        "This cell securely initializes the OpenAI client. If the `OPENAI_API_KEY` environment variable is not present, the cell prompts for a key using a hidden input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d2147149",
      "metadata": {
        "id": "d2147149",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "853812d6-ac04-444e-ae74-615f5da71a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI client initialized.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "from openai import OpenAI\n",
        "\n",
        "# Attempt to load key from environment. Prompt if missing.\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    print(\"OpenAI API key not found in environment.\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI API key (input hidden): \").strip()\n",
        "\n",
        "# Initialize client (will raise if key invalid)\n",
        "client = OpenAI()\n",
        "print(\"OpenAI client initialized.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2ead9ad",
      "metadata": {
        "id": "f2ead9ad"
      },
      "source": [
        "\n",
        "## Step 1 ‚Äì ASR and Translation <a id=\"asr-translation\"></a>\n",
        "This section performs automatic speech recognition (ASR) on Spanish audio using Whisper (**base** model), followed by English translation via the OpenAI API.\n",
        "\n",
        "**Inputs**: `.wav` files under `AUDIO_DIR`.  \n",
        "**Outputs**: `audio_translations.csv` with columns: `audio_file`, `spanish_transcription`, `english_translation`, `language_detected`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "0128eb68",
      "metadata": {
        "id": "0128eb68"
      },
      "outputs": [],
      "source": [
        "\n",
        "import whisper\n",
        "import pandas as pd\n",
        "\n",
        "def transcribe_spanish_audio(model, audio_path: Path):\n",
        "    \"\"\"\n",
        "    Transcribe a single Spanish audio file using Whisper.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : whisper.Whisper\n",
        "        Loaded Whisper model instance.\n",
        "    audio_path : Path\n",
        "        Path to the .wav file.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    text : str\n",
        "        Detected transcription text.\n",
        "    lang : str\n",
        "        Detected language code.\n",
        "    \"\"\"\n",
        "    result = model.transcribe(str(audio_path), language=\"es\", task=\"transcribe\", verbose=False)\n",
        "    return result[\"text\"].strip(), result.get(\"language\", \"unknown\")\n",
        "\n",
        "\n",
        "def translate_spanish_to_english(spanish_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Translate a Spanish transcription to English via OpenAI chat completion.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    spanish_text : str\n",
        "        Input Spanish text to translate.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    english_text : str\n",
        "        Translated English text.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"Translate the following Spanish medical transcription into clear, faithful English:\\n\\n\"\n",
        "        + spanish_text\n",
        "    )\n",
        "    result = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        temperature=0,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "    return result.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "def process_and_translate_audio(audio_folder: Path, output_csv: Path, model_size: str = \"base\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Run Whisper ASR on all .wav files in a folder, translate to English, and save results.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    audio_folder : Path\n",
        "        Directory containing .wav files.\n",
        "    output_csv : Path\n",
        "        Destination CSV for transcriptions and translations.\n",
        "    model_size : str, optional\n",
        "        Whisper model size (default: \"base\").\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame of results with columns:\n",
        "        [audio_file, spanish_transcription, english_translation, language_detected].\n",
        "    \"\"\"\n",
        "    model = whisper.load_model(model_size)\n",
        "    audio_files = sorted([f for f in os.listdir(audio_folder) if f.endswith(\".wav\")])\n",
        "\n",
        "    results = []\n",
        "    for fname in audio_files:\n",
        "        audio_path = audio_folder / fname\n",
        "        if not audio_path.exists():\n",
        "            continue\n",
        "        es_text, detected_lang = transcribe_spanish_audio(model, audio_path)\n",
        "        en_text = translate_spanish_to_english(es_text)\n",
        "        results.append({\n",
        "            \"audio_file\": fname,\n",
        "            \"spanish_transcription\": es_text,\n",
        "            \"english_translation\": en_text,\n",
        "            \"language_detected\": detected_lang\n",
        "        })\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
        "    print(\"Saved:\", output_csv)\n",
        "    return df\n",
        "\n",
        "# Example (do not auto-run):\n",
        "# trans_csv = LLM_OUT / \"audio_translations.csv\"\n",
        "# _ = process_and_translate_audio(AUDIO_DIR, trans_csv, model_size=\"base\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c531344",
      "metadata": {
        "id": "8c531344"
      },
      "source": [
        "\n",
        "## Step 2 ‚Äì ASR Evaluation (Input) <a id=\"asr-eval\"></a>\n",
        "This section computes WER and CER for the input ASR stage by aligning Whisper transcriptions with ground-truth text.\n",
        "\n",
        "**Assumptions**  \n",
        "- Ground truth is provided in `CSV_DIR / \"ground_truth.csv\"` with columns: `audio_file`, `ground_truth`.\n",
        "- Transcriptions are in `LLM_OUT / \"audio_translations.csv\"` with columns: `audio_file`, `spanish_transcription`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "24d16763",
      "metadata": {
        "id": "24d16763"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from jiwer import wer, cer\n",
        "\n",
        "def evaluate_asr_performance(ground_truth_csv: Path, trans_csv: Path, save_csv: Path) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compute WER and CER for input ASR against ground truth.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ground_truth_csv : Path\n",
        "        CSV file with columns: [audio_file, ground_truth].\n",
        "    trans_csv : Path\n",
        "        CSV file with columns: [audio_file, spanish_transcription].\n",
        "    save_csv : Path\n",
        "        Destination CSV for ASR metrics.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Evaluation results with per-file WER/CER.\n",
        "    \"\"\"\n",
        "    gt_df = pd.read_csv(ground_truth_csv)\n",
        "    tr_df = pd.read_csv(trans_csv)\n",
        "\n",
        "    # Defensive renaming for common variations.\n",
        "    gt_df = gt_df.rename(columns={\"filename\": \"audio_file\", \"spanish_text\": \"ground_truth\"})\n",
        "    tr_df = tr_df.rename(columns={\"transcription\": \"spanish_transcription\"})\n",
        "\n",
        "    df = pd.merge(tr_df[[\"audio_file\", \"spanish_transcription\"]],\n",
        "                  gt_df[[\"audio_file\", \"ground_truth\"]],\n",
        "                  on=\"audio_file\", how=\"inner\")\n",
        "\n",
        "    df[\"WER\"] = [wer(ref, hyp) for ref, hyp in zip(df[\"ground_truth\"], df[\"spanish_transcription\"])]\n",
        "    df[\"CER\"] = [cer(ref, hyp) for ref, hyp in zip(df[\"ground_truth\"], df[\"spanish_transcription\"])]\n",
        "\n",
        "    df.to_csv(save_csv, index=False)\n",
        "    print(\"Saved:\", save_csv)\n",
        "    return df\n",
        "\n",
        "# Example (do not auto-run):\n",
        "# gt_csv  = CSV_DIR / \"ground_truth.csv\"\n",
        "# trans_csv = LLM_OUT / \"audio_translations.csv\"\n",
        "# asr_csv = EVAL_DIR / \"asr_metrics.csv\"\n",
        "# _ = evaluate_asr_performance(gt_csv, trans_csv, asr_csv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4477ecb",
      "metadata": {
        "id": "f4477ecb"
      },
      "source": [
        "\n",
        "## Step 3 ‚Äì LLM Question Answering <a id=\"qa\"></a>\n",
        "This section queries an LLM with English questions derived from the ASR+translation step and provides answers based on a tabular blood-pressure dataset.\n",
        "\n",
        "**Inputs**: A CSV file with synthetic blood-pressure records.  \n",
        "**Outputs**: English answers and associated computed fields.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "85f89584",
      "metadata": {
        "id": "85f89584"
      },
      "outputs": [],
      "source": [
        "\n",
        "def ask_gpt(question_en: str, csv_block: str) -> dict:\n",
        "    \"\"\"\n",
        "    Query the LLM with a question and a CSV context block.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    question_en : str\n",
        "        English question derived from Spanish transcription.\n",
        "    csv_block : str\n",
        "        CSV content as a single string for in-context grounding.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        A dictionary with keys:\n",
        "        - \"answer\": str (LLM's English answer)\n",
        "        - \"computed_fields\": dict (optional structured fields)\n",
        "    \"\"\"\n",
        "\n",
        "    system = \"\"\"\n",
        "You are a careful and detail-oriented data analyst.\n",
        "\n",
        "You are given a synthetic blood pressure dataset in CSV format. It contains readings for one individual over the last 30 consecutive days, with the following columns:\n",
        "\n",
        "- date\n",
        "- age\n",
        "- sex\n",
        "- systolic_mmHg\n",
        "- diastolic_mmHg\n",
        "\n",
        "Use only the data in the CSV to answer all questions, except when normal blood pressure ranges are requested ‚Äî in those cases, you may use external references but must cite your source.\n",
        "\n",
        "---\n",
        "\n",
        "üß† Interpretation Guidelines:\n",
        "\n",
        "- \"Today\" refers to the most recent date in the dataset.\n",
        "- \"Yesterday\" means the most recent date before \"today\", based on available data.\n",
        "- Phrases like \"last week\" or \"last month\" refer to calendar-based timeframes (e.g., the 7 or 30 days before \"today\"), not just row counts.\n",
        "- If a question refers to a specific date or date range that is not present in the dataset, clearly state that the data is unavailable.\n",
        "- Use conversational date formats like ‚ÄúOctober 12‚Äù or ‚ÄúOctober 12 to 15‚Äù ‚Äî avoid numeric formats like ‚Äú10/12/2025‚Äù.\n",
        "\n",
        "---\n",
        "\n",
        "üí¨ Answer Style:\n",
        "\n",
        "- Write in natural, conversational English.\n",
        "- Address the user directly using ‚Äúyou‚Äù (e.g., ‚ÄúYour blood pressure was‚Ä¶‚Äù).\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ Response Format:\n",
        "\n",
        "Return all answers in the following JSON format:\n",
        "\n",
        "{ \"answer\": \"<English answer>\", \"computed_fields\": { \"numeric values used\" } }\n",
        "\n",
        "---\n",
        "\n",
        "üìå Example Questions and Answers:\n",
        "\n",
        "These illustrate tone and structure only. Actual answers must be based on the CSV data.\n",
        "\n",
        "Q: What are my systolic_mmHg and diastolic_mmHg blood pressures today?\n",
        "A: Your systolic blood pressure was [xx] mm Hg and your diastolic pressure was [yy] mm Hg today.\n",
        "\n",
        "Q: What were the values over the last week?\n",
        "A: Over the last 7 days, your systolic pressure averaged [xx] mm Hg and your diastolic pressure averaged [yy] mm Hg.\n",
        "\n",
        "Q: What is the trend of my blood pressure?\n",
        "A: Your blood pressure has shown a gradual increase in systolic values over the last 30 days, while your diastolic readings have remained stable.\n",
        "\n",
        "Q: What are the normal ranges for a person like me?\n",
        "A: Based on your age ([age from dataset] years) and sex ([male/female]), typical blood pressure values are approximately [xx/yy] mm Hg, according to [name and link to the external source].\n",
        "\"\"\"\n",
        "\n",
        "    user = (\n",
        "        f\"CSV:\\n{csv_block}\\n\\n\"\n",
        "        f\"Question:\\n{question_en}\\n\\n\"\n",
        "        \"Please analyze and return the response strictly following the JSON format defined above.\"\n",
        "    )\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        temperature=0,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user},\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    answer_text = response.choices[0].message.content.strip()\n",
        "\n",
        "    # Ensure output is JSON-safe\n",
        "    try:\n",
        "        result = json.loads(answer_text)\n",
        "    except json.JSONDecodeError:\n",
        "        result = {\"answer\": answer_text, \"computed_fields\": {}}\n",
        "\n",
        "    return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b90c1a9b",
      "metadata": {
        "id": "b90c1a9b"
      },
      "source": [
        "\n",
        "## Step 4 ‚Äì Translation and TTS <a id=\"tts\"></a>\n",
        "This section back-translates the LLM's English answers to Spanish and generates Spanish audio (TTS).\n",
        "\n",
        "Note: A simple gTTS-based fallback is provided (exports `.wav` via pydub). If your project includes a custom TTS, replace the fallback with your implementation and keep the same function signature.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "589d5361",
      "metadata": {
        "id": "589d5361"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "\n",
        "def translate_to_spanish(text_en: str) -> str:\n",
        "    \"\"\"\n",
        "    Translate English text to Spanish using OpenAI chat completion.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text_en : str\n",
        "        English text to translate.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Spanish translation.\n",
        "    \"\"\"\n",
        "    prompt = \"Translate the following English medical answer into Spanish:\\n\\n\" + text_en\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        temperature=0,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "def text_to_speech_spanish(text_es: str, out_wav_path: Path):\n",
        "    \"\"\"\n",
        "    Generate Spanish speech from text.\n",
        "\n",
        "    This fallback uses gTTS + pydub to export a WAV file if a custom TTS is\n",
        "    not available. Replace this function with your project-specific TTS if needed.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text_es : str\n",
        "        Spanish text to synthesize.\n",
        "    out_wav_path : Path\n",
        "        Destination path for the WAV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from gtts import gTTS\n",
        "        from pydub import AudioSegment\n",
        "        tmp_mp3 = out_wav_path.with_suffix(\".mp3\")\n",
        "        gTTS(text_es, lang=\"es\").save(tmp_mp3)\n",
        "        # Convert to WAV\n",
        "        audio = AudioSegment.from_file(tmp_mp3, format=\"mp3\")\n",
        "        audio.export(out_wav_path, format=\"wav\")\n",
        "        os.remove(tmp_mp3)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"TTS fallback failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af0262a4",
      "metadata": {
        "id": "af0262a4"
      },
      "source": [
        "\n",
        "## Step 5 ‚Äì Whisper Evaluation of TTS <a id=\"tts-eval\"></a>\n",
        "This section re-transcribes the generated Spanish audio responses using Whisper (base) and evaluates intelligibility against the ground-truth Spanish answers using WER, CER, and SER.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "f48ecab5",
      "metadata": {
        "id": "f48ecab5"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re, unicodedata\n",
        "import Levenshtein\n",
        "from jiwer import process_words\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize text for fair ASR comparison.\n",
        "    - Lowercase\n",
        "    - Strip accents\n",
        "    - Remove punctuation\n",
        "    - Collapse extra whitespace\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def evaluate_output_asr_whisper(\n",
        "    tts_csv: Path,\n",
        "    output_csv: Path = None,\n",
        "    model_size: str = \"base\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Evaluate Spanish TTS audios using Whisper by computing WER, CER, and SER\n",
        "    against ground-truth Spanish answers.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    tts_csv : Path\n",
        "        CSV with columns: [spanish_answer, audio_answer_file].\n",
        "    output_csv : Path, optional\n",
        "        Destination CSV for ASR metrics (defaults to EVAL_DIR / \"output_asr_metrics_whisper.csv\").\n",
        "    model_size : str, optional\n",
        "        Whisper model size (default: \"base\").\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame with per-file metrics.\n",
        "    \"\"\"\n",
        "    if output_csv is None:\n",
        "        output_csv = EVAL_DIR / \"output_asr_metrics_whisper.csv\"\n",
        "\n",
        "    if not tts_csv.exists():\n",
        "        raise FileNotFoundError(f\"Missing final results CSV: {tts_csv}\")\n",
        "\n",
        "    model = whisper.load_model(model_size)\n",
        "\n",
        "    df = pd.read_csv(tts_csv)\n",
        "    rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        gt = str(row.get(\"spanish_answer\", \"\")).strip()\n",
        "        audio_file = str(row.get(\"audio_answer_file\", \"\")).strip()\n",
        "        if not gt or not audio_file or not os.path.exists(audio_file):\n",
        "            continue\n",
        "\n",
        "        # Transcribe generated audio and normalize\n",
        "        res = model.transcribe(audio_file, language=\"es\", task=\"transcribe\", verbose=False)\n",
        "        hyp = res.get(\"text\", \"\").strip()\n",
        "\n",
        "        gt_norm = normalize_text(gt)\n",
        "        hyp_norm = normalize_text(hyp)\n",
        "\n",
        "        measures = process_words(gt_norm, hyp_norm)\n",
        "        wer_score = round(measures.wer, 4)\n",
        "        subs, dels, ins = measures.substitutions, measures.deletions, measures.insertions\n",
        "        cer_score = round(Levenshtein.distance(gt_norm, hyp_norm) / max(len(gt_norm), 1), 4)\n",
        "        ser_score = 0 if gt_norm == hyp_norm else 1\n",
        "\n",
        "        rows.append({\n",
        "            \"audio_file\": os.path.basename(audio_file),\n",
        "            \"ground_truth\": gt,\n",
        "            \"whisper_transcription\": hyp,\n",
        "            \"WER\": wer_score,\n",
        "            \"Substitutions\": subs,\n",
        "            \"Deletions\": dels,\n",
        "            \"Insertions\": ins,\n",
        "            \"CER\": cer_score,\n",
        "            \"SER\": ser_score,\n",
        "        })\n",
        "\n",
        "    out_df = pd.DataFrame(rows)\n",
        "    out_df.to_csv(output_csv, index=False)\n",
        "    print(\"Saved:\", output_csv)\n",
        "    return out_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9911afe5",
      "metadata": {
        "id": "9911afe5"
      },
      "source": [
        "\n",
        "## Orchestration (Optional) <a id=\"orchestration\"></a>\n",
        "The following function orchestrates all steps in sequence. Each step can also be run individually.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "852067e6",
      "metadata": {
        "id": "852067e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "017889af-492a-4531-880a-db3a3c9a7734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 458/458 [00:01<00:00, 242.60frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 470/470 [00:01<00:00, 266.51frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 369/369 [00:01<00:00, 209.37frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 398/398 [00:01<00:00, 212.16frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 328/328 [00:01<00:00, 191.48frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1574/1574 [00:02<00:00, 558.36frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/HealthTequity-LLM/results/llm_outputs/audio_translations.csv\n",
            "Saved: /content/drive/MyDrive/HealthTequity-LLM/results/evaluation_metrics/asr_metrics.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n",
            "  m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n",
            "  m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n",
            "  elif re.match('(flt)p?( \\(default\\))?$', token):\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n",
            "  elif re.match('(dbl)p?( \\(default\\))?$', token):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/HealthTequity-LLM/results/llm_outputs/final_pipeline_results.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1099/1099 [00:02<00:00, 438.77frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1305/1305 [00:03<00:00, 357.46frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4099/4099 [00:06<00:00, 648.85frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1790/1790 [00:04<00:00, 413.96frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1221/1221 [00:02<00:00, 514.08frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6192/6192 [00:09<00:00, 655.82frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/HealthTequity-LLM/results/evaluation_metrics/output_asr_metrics_whisper.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "def run_full_pipeline(csv_path: Path, audio_folder: Path, whisper_model_size: str = \"base\"):\n",
        "    \"\"\"\n",
        "    Execute the full pipeline, from input ASR/translation to TTS evaluation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    csv_path : Path\n",
        "        Path to the synthetic blood-pressure CSV.\n",
        "    audio_folder : Path\n",
        "        Directory containing input Spanish .wav files.\n",
        "    whisper_model_size : str, optional\n",
        "        Whisper model size, default \"base\".\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Summary dictionary with output artifact locations.\n",
        "    \"\"\"\n",
        "    # Step 1: ASR + Translation\n",
        "    trans_csv = LLM_OUT / \"audio_translations.csv\"\n",
        "    _ = process_and_translate_audio(audio_folder, trans_csv, model_size=whisper_model_size)\n",
        "\n",
        "    # Step 2: Evaluate input ASR\n",
        "    gt_csv = CSV_DIR / \"ground_truth.csv\"\n",
        "    asr_csv = EVAL_DIR / \"asr_metrics.csv\"\n",
        "    _ = evaluate_asr_performance(gt_csv, trans_csv, asr_csv)\n",
        "\n",
        "    # Step 3: Load tabular data for LLM grounding\n",
        "    df_bp = pd.read_csv(csv_path)\n",
        "    csv_block = df_bp.to_csv(index=False)\n",
        "\n",
        "    # Step 4: Q&A + Spanish TTS\n",
        "    results = []\n",
        "    tr_df = pd.read_csv(trans_csv)\n",
        "    for i, row in tr_df.iterrows():\n",
        "        q_en = row[\"english_translation\"]\n",
        "        ans = ask_gpt(q_en, csv_block)\n",
        "        ans_en = ans.get(\"answer\", \"\").strip()\n",
        "        ans_es = translate_to_spanish(ans_en)\n",
        "\n",
        "        out_wav = TTS_DIR / f\"answer_{i+1}_es.wav\"\n",
        "        text_to_speech_spanish(ans_es, out_wav)\n",
        "\n",
        "        results.append({\n",
        "            \"question_number\": i + 1,\n",
        "            \"audio_file_in\": row[\"audio_file\"],\n",
        "            \"spanish_question\": row[\"spanish_transcription\"],\n",
        "            \"english_question\": q_en,\n",
        "            \"english_answer\": ans_en,\n",
        "            \"spanish_answer\": ans_es,\n",
        "            \"audio_answer_file\": str(out_wav),\n",
        "            \"computed_fields\": json.dumps(ans.get(\"computed_fields\", {}))\n",
        "        })\n",
        "\n",
        "    final_csv = LLM_OUT / \"final_pipeline_results.csv\"\n",
        "    pd.DataFrame(results).to_csv(final_csv, index=False)\n",
        "    print(\"Saved:\", final_csv)\n",
        "\n",
        "    # Step 5: Evaluate TTS intelligibility\n",
        "    output_asr_csv = EVAL_DIR / \"output_asr_metrics_whisper.csv\"\n",
        "    _ = evaluate_output_asr_whisper(final_csv, output_csv=output_asr_csv, model_size=whisper_model_size)\n",
        "\n",
        "    return {\n",
        "        \"transcriptions_csv\": str(trans_csv),\n",
        "        \"input_asr_metrics_csv\": str(asr_csv),\n",
        "        \"final_pipeline_csv\": str(final_csv),\n",
        "        \"output_asr_metrics_csv\": str(output_asr_csv),\n",
        "    }\n",
        "\n",
        "# Example (do not auto-run):\n",
        "bp_csv = CSV_DIR / \"synthetic_bp_one_person.csv\"\n",
        "_ = run_full_pipeline(bp_csv, AUDIO_DIR, whisper_model_size=\"base\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b1fc405",
      "metadata": {
        "id": "5b1fc405"
      },
      "source": [
        "\n",
        "## Step 6 ‚Äì Results Summary <a id=\"summary\"></a>\n",
        "This section summarizes average WER, CER, and SER across input ASR and TTS evaluation outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "23c1ed41",
      "metadata": {
        "id": "23c1ed41"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "def summarize_results(input_asr_csv: Path, output_asr_csv: Path):\n",
        "    \"\"\"\n",
        "    Print dataset-level average metrics for input ASR and TTS ASR evaluation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_asr_csv : Path\n",
        "        CSV with per-file input ASR metrics.\n",
        "    output_asr_csv : Path\n",
        "        CSV with per-file TTS ASR metrics.\n",
        "    \"\"\"\n",
        "    print(\"Input ASR metrics (WER, CER):\")\n",
        "    if Path(input_asr_csv).exists():\n",
        "        d1 = pd.read_csv(input_asr_csv)\n",
        "        print(d1[[\"WER\", \"CER\", \"SER\"]].mean(numeric_only=True).to_frame(\"Average\"))\n",
        "    else:\n",
        "        print(\"Missing:\", input_asr_csv)\n",
        "\n",
        "    print(\"\\nTTS ASR metrics (WER, CER, SER):\")\n",
        "    if Path(output_asr_csv).exists():\n",
        "        d2 = pd.read_csv(output_asr_csv)\n",
        "        print(d2[[\"WER\", \"CER\", \"SER\"]].mean(numeric_only=True).to_frame(\"Average\"))\n",
        "    else:\n",
        "        print(\"Missing:\", output_asr_csv)\n",
        "\n",
        "# Example (do not auto-run):\n",
        "# summarize_results(EVAL_DIR / \"asr_metrics.csv\", EVAL_DIR / \"output_asr_metrics_whisper.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "summarize_results(EVAL_DIR / \"asr_metrics.csv\", EVAL_DIR / \"output_asr_metrics_whisper.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jXT7dn1Nz8D",
        "outputId": "7704d700-9e0c-41a6-ff15-71acccf9db5e"
      },
      "id": "1jXT7dn1Nz8D",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input ASR metrics (WER, CER):\n",
            "      Average\n",
            "WER  0.035183\n",
            "CER  0.005517\n",
            "SER  0.333333\n",
            "\n",
            "TTS ASR metrics (WER, CER, SER):\n",
            "      Average\n",
            "WER  0.211000\n",
            "CER  0.160367\n",
            "SER  1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c776ee9f",
      "metadata": {
        "id": "c776ee9f"
      },
      "source": [
        "\n",
        "## Appendix <a id=\"appendix\"></a>\n",
        "- All paths are centralized under `PROJECT_ROOT` for reproducibility.  \n",
        "- Replace the TTS fallback with a project-specific implementation if available.  \n",
        "- The Whisper model size can be adjusted by changing `whisper_model_size` in the orchestration call.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}