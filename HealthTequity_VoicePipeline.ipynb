{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nattaran/HealthTequity-LLM/blob/main/HealthTequity_VoicePipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f46ec9ac",
      "metadata": {
        "id": "f46ec9ac"
      },
      "source": [
        "\n",
        "# HealthTequity Voice Pipeline\n",
        "\n",
        "## Introduction <a id=\"introduction\"></a>\n",
        "This notebook presents a modular voice pipeline for bilingual (Spanishâ€“English) processing and evaluation. The system performs the following sequence:\n",
        "1) Automatic Speech Recognition (ASR) on Spanish audio inputs (Whisper base).  \n",
        "2) Machine translation (Spanish â†’ English) for downstream analysis.  \n",
        "3) Question answering over a tabular blood-pressure dataset using an LLM (GPT-40).  \n",
        "4) Back-translation to Spanish followed by text-to-speech (TTS).  \n",
        "5) Re-transcription of the generated Spanish audio using ASR and evaluation (WER, CER, SER).\n",
        "\n",
        "All steps are clearly separated, reproducible, and designed to be executed independently.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9ecd8b4",
      "metadata": {
        "id": "b9ecd8b4"
      },
      "source": [
        "\n",
        "## Table of Contents\n",
        "1. [Introduction](#introduction)  \n",
        "2. [Environment Setup](#setup)  \n",
        "3. [Folder Configuration](#paths)  \n",
        "4. [OpenAI Key Initialization](#openai)  \n",
        "5. [Step 1 â€“ ASR and Translation](#asr-translation)  \n",
        "6. [Step 2 â€“ ASR Evaluation](#asr-eval)  \n",
        "7. [Step 3 â€“ LLM Question Answering](#qa)  \n",
        "8. [Step 4 â€“ Translation and TTS](#tts)  \n",
        "9. [Step 5 â€“ Whisper Evaluation of TTS](#tts-eval)  \n",
        "10. [Step 6 â€“ Results Summary](#summary)  \n",
        "11. [Appendix](#appendix)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the mountpoint\n",
        "mountpoint = '/content/drive'\n",
        "\n",
        "# Check if the mountpoint exists and is not empty, then clear it\n",
        "if os.path.exists(mountpoint) and os.path.isdir(mountpoint) and os.listdir(mountpoint):\n",
        "    print(f\"Clearing mountpoint: {mountpoint}\")\n",
        "    try:\n",
        "        # Use shutil.rmtree to remove the directory and its contents\n",
        "        shutil.rmtree(mountpoint)\n",
        "        # Recreate the directory after removal\n",
        "        os.makedirs(mountpoint)\n",
        "    except Exception as e:\n",
        "        print(f\"Error clearing mountpoint: {e}\")\n",
        "        # If clearing fails, you might want to handle this differently,\n",
        "        # perhaps by raising an error or trying a different mountpoint.\n",
        "        # For now, we'll print an error and proceed, which might lead to\n",
        "        # the same ValueError again, but shows the attempt to clear.\n",
        "        pass\n",
        "\n",
        "\n",
        "drive.mount(mountpoint, force_remount=True)\n",
        "\n",
        "# Clone or update repo inside Drive\n",
        "repo_url = \"https://github.com/nattaran/HealthTequity-LLM.git\"\n",
        "repo_path = \"/content/drive/MyDrive/HealthTequity-LLM\"\n",
        "\n",
        "\n",
        "if not os.path.exists(repo_path):\n",
        "    !git clone {repo_url} {repo_path}\n",
        "else:\n",
        "    %cd {repo_path}\n",
        "    !git fetch origin\n",
        "    !git pull\n",
        "\n",
        "%cd {repo_path}\n",
        "print(\"âœ… Environment ready. Working from:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9GGDiUrHnS2",
        "outputId": "e4eb7e6a-85f8-4acd-87b4-4939a6720bd3"
      },
      "id": "R9GGDiUrHnS2",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/HealthTequity-LLM\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 54 (delta 3), reused 3 (delta 3), pack-reused 48 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (54/54), 560.59 KiB | 185.00 KiB/s, done.\n",
            "From https://github.com/nattaran/HealthTequity-LLM\n",
            "   e2ac919..2a6b7ad  main       -> origin/main\n",
            "Updating e2ac919..2a6b7ad\n",
            "error: Your local changes to the following files would be overwritten by merge:\n",
            "\tdata/Spanish_audio/q2_es.wav\n",
            "\tdata/Spanish_audio/q3_es.wav\n",
            "\tdata/Spanish_audio/q4_es.wav\n",
            "\tdata/Spanish_audio/q5_es.wav\n",
            "\tdata/Spanish_audio/q6_es.wav\n",
            "\tdata/synthetic_csv/ground_truth.csv\n",
            "Please commit your changes or stash them before you merge.\n",
            "error: The following untracked working tree files would be overwritten by merge:\n",
            "\tdata/Spanish_audio/q10_es.wav\n",
            "\tdata/Spanish_audio/q7_es.wav\n",
            "\tdata/Spanish_audio/q8_es.wav\n",
            "\tdata/Spanish_audio/q9_es.wav\n",
            "\tdata/synthetic_csv/synthetic_bp_one_person.csv\n",
            "Please move or remove them before you merge.\n",
            "Aborting\n",
            "/content/drive/MyDrive/HealthTequity-LLM\n",
            "âœ… Environment ready. Working from: /content/drive/MyDrive/HealthTequity-LLM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23d83703",
      "metadata": {
        "id": "23d83703"
      },
      "source": [
        "\n",
        "## Environment Setup <a id=\"setup\"></a>\n",
        "Install dependencies from the provided `requirements.txt`. Run this cell once per runtime.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a53aba9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a53aba9f",
        "outputId": "d927d7b6-f15f-4faf-b38d-61377020e79c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git (from -r requirements.txt (line 22))\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-ek22h6vr\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-ek22h6vr\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (2.2.2)\n",
            "Requirement already satisfied: matplotlib>=3.8.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (3.10.0)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (2.8.0+cu126)\n",
            "Requirement already satisfied: openai>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 15)) (1.109.1)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 16)) (0.12.0)\n",
            "Collecting whisper (from -r requirements.txt (line 20))\n",
            "  Downloading whisper-1.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ffmpeg-python>=0.2.0 (from -r requirements.txt (line 25))\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 26)) (0.13.1)\n",
            "Requirement already satisfied: librosa>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 27)) (0.11.0)\n",
            "Requirement already satisfied: pydub>=0.25.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 28)) (0.25.1)\n",
            "Collecting jiwer>=3.0.3 (from -r requirements.txt (line 31))\n",
            "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting python-Levenshtein>=0.25.0 (from -r requirements.txt (line 32))\n",
            "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting gtts>=2.3.2 (from -r requirements.txt (line 35))\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting deep-translator>=1.11.4 (from -r requirements.txt (line 36))\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: python-dotenv>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 39)) (1.1.1)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 40)) (4.67.1)\n",
            "Collecting vosk==0.3.45 (from -r requirements.txt (line 43))\n",
            "  Downloading vosk-0.3.45-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from vosk==0.3.45->-r requirements.txt (line 43)) (2.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vosk==0.3.45->-r requirements.txt (line 43)) (2.32.4)\n",
            "Collecting srt (from vosk==0.3.45->-r requirements.txt (line 43))\n",
            "  Downloading srt-3.5.3.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.12/dist-packages (from vosk==0.3.45->-r requirements.txt (line 43)) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->-r requirements.txt (line 7)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->-r requirements.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->-r requirements.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (3.2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (3.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (1.3.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.6.0->-r requirements.txt (line 16)) (2024.11.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from whisper->-r requirements.txt (line 20)) (1.17.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625->-r requirements.txt (line 22)) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625->-r requirements.txt (line 22)) (0.60.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from ffmpeg-python>=0.2.0->-r requirements.txt (line 25)) (1.0.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.16.2)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.0.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.1.2)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer>=3.0.3->-r requirements.txt (line 31)) (8.3.0)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer>=3.0.3->-r requirements.txt (line 31))\n",
            "  Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Collecting Levenshtein==0.27.1 (from python-Levenshtein>=0.25.0->-r requirements.txt (line 32))\n",
            "  Downloading levenshtein-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting click>=8.1.8 (from jiwer>=3.0.3->-r requirements.txt (line 31))\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.12/dist-packages (from deep-translator>=1.11.4->-r requirements.txt (line 36)) (4.13.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai>=1.12.0->-r requirements.txt (line 15)) (3.11)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator>=1.11.4->-r requirements.txt (line 36)) (2.8)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->vosk==0.3.45->-r requirements.txt (line 43)) (2.23)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=1.12.0->-r requirements.txt (line 15)) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=1.12.0->-r requirements.txt (line 15)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.12.0->-r requirements.txt (line 15)) (0.16.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper==20250625->-r requirements.txt (line 22)) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa>=0.10.1->-r requirements.txt (line 27)) (4.5.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.12.0->-r requirements.txt (line 15)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.12.0->-r requirements.txt (line 15)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.12.0->-r requirements.txt (line 15)) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vosk==0.3.45->-r requirements.txt (line 43)) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vosk==0.3.45->-r requirements.txt (line 43)) (2.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa>=0.10.1->-r requirements.txt (line 27)) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.0->-r requirements.txt (line 11)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.0->-r requirements.txt (line 11)) (3.0.3)\n",
            "Downloading vosk-0.3.45-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
            "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (159 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: whisper, openai-whisper, srt\n",
            "  Building wheel for whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for whisper: filename=whisper-1.1.10-py3-none-any.whl size=41120 sha256=b5185ff77aac852286104ed8bf90b0d50d196b0f9930839766baf902a4d2222a\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/b8/4e/9c4c3351d670e06746a340fb4b7d854c76517eec225e5b32b1\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=7906970dc5ec1c85b656eb483ee6ba967c71727876afa546e6771d0e35cbc5ca\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8dx00eab/wheels/c3/03/25/5e0ba78bc27a3a089f137c9f1d92fdfce16d06996c071a016c\n",
            "  Building wheel for srt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for srt: filename=srt-3.5.3-py3-none-any.whl size=22427 sha256=94469eebcf59b3f01b4170f73398f7b336569ea7fec16ff29bde70b69ca05480\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/75/5b/e1d5c3756631e4bda806f6cc9640153b39484bb6f7b0b8def3\n",
            "Successfully built whisper openai-whisper srt\n",
            "Installing collected packages: whisper, srt, rapidfuzz, ffmpeg-python, click, vosk, Levenshtein, jiwer, gtts, deep-translator, python-Levenshtein, openai-whisper\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.3.0\n",
            "    Uninstalling click-8.3.0:\n",
            "      Successfully uninstalled click-8.3.0\n",
            "Successfully installed Levenshtein-0.27.1 click-8.1.8 deep-translator-1.11.4 ffmpeg-python-0.2.0 gtts-2.5.4 jiwer-4.0.0 openai-whisper-20250625 python-Levenshtein-0.27.1 rapidfuzz-3.14.1 srt-3.5.3 vosk-0.3.45 whisper-1.1.10\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install project requirements (run once per session)\n",
        "# If you prefer to pin versions, ensure requirements.txt includes exact versions.\n",
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2af3db58",
      "metadata": {
        "id": "2af3db58"
      },
      "source": [
        "\n",
        "## Folder Configuration <a id=\"paths\"></a>\n",
        "Centralized path configuration for data and results. This version assumes a Drive-based working directory that persists across sessions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b5ec22c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5ec22c8",
        "outputId": "fd8aa068-3343-4f40-b01e-0df1ee774776"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project root: /content/drive/MyDrive/HealthTequity-LLM\n",
            "Data dir: /content/drive/MyDrive/HealthTequity-LLM/data\n",
            "CSV dir: /content/drive/MyDrive/HealthTequity-LLM/data/synthetic_csv\n",
            "Audio dir: /content/drive/MyDrive/HealthTequity-LLM/data/Spanish_audio\n",
            "Results dir: /content/drive/MyDrive/HealthTequity-LLM/results\n",
            "LLM outputs: /content/drive/MyDrive/HealthTequity-LLM/results/llm_outputs\n",
            "Evaluation dir: /content/drive/MyDrive/HealthTequity-LLM/results/evaluation_metrics\n",
            "TTS dir: /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Project root in Google Drive (adjust if needed)\n",
        "PROJECT_ROOT = Path(\"/content/drive/MyDrive/HealthTequity-LLM\")\n",
        "\n",
        "# Data and results subfolders\n",
        "DATA_DIR     = PROJECT_ROOT / \"data\"\n",
        "CSV_DIR      = DATA_DIR / \"synthetic_csv\"\n",
        "AUDIO_DIR    = DATA_DIR / \"Spanish_audio\"\n",
        "RESULTS_DIR  = PROJECT_ROOT / \"results\"\n",
        "LLM_OUT      = RESULTS_DIR / \"llm_outputs\"\n",
        "EVAL_DIR     = RESULTS_DIR / \"evaluation_metrics\"\n",
        "TTS_DIR      = RESULTS_DIR / \"tts_audio\"\n",
        "\n",
        "# Create outputs if missing (idempotent)\n",
        "for p in [RESULTS_DIR, LLM_OUT, EVAL_DIR, TTS_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Project root:\", PROJECT_ROOT)\n",
        "print(\"Data dir:\", DATA_DIR)\n",
        "print(\"CSV dir:\", CSV_DIR)\n",
        "print(\"Audio dir:\", AUDIO_DIR)\n",
        "print(\"Results dir:\", RESULTS_DIR)\n",
        "print(\"LLM outputs:\", LLM_OUT)\n",
        "print(\"Evaluation dir:\", EVAL_DIR)\n",
        "print(\"TTS dir:\", TTS_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daf53a53",
      "metadata": {
        "id": "daf53a53"
      },
      "source": [
        "\n",
        "## OpenAI Key Initialization <a id=\"openai\"></a>\n",
        "This cell securely initializes the OpenAI client. If the `OPENAI_API_KEY` environment variable is not present, the cell prompts for a key using a hidden input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d2147149",
      "metadata": {
        "id": "d2147149",
        "outputId": "04c481fb-6d94-459a-bf9f-ee54a238b19a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key not found in environment.\n",
            "Paste your OpenAI API key (input hidden): Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "OpenAI client initialized.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "from openai import OpenAI\n",
        "\n",
        "# Attempt to load key from environment. Prompt if missing.\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    print(\"OpenAI API key not found in environment.\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI API key (input hidden): \").strip()\n",
        "\n",
        "# Initialize client (will raise if key invalid)\n",
        "client = OpenAI()\n",
        "print(\"OpenAI client initialized.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2ead9ad",
      "metadata": {
        "id": "f2ead9ad"
      },
      "source": [
        "\n",
        "## Step 1 â€“ ASR and Translation <a id=\"asr-translation\"></a>\n",
        "This section performs automatic speech recognition (ASR) on Spanish audio using Whisper (**base** model), followed by English translation via the OpenAI API.\n",
        "\n",
        "**Inputs**: `.wav` files under `AUDIO_DIR`.  \n",
        "**Outputs**: `audio_translations.csv` with columns: `audio_file`, `spanish_transcription`, `english_translation`, `language_detected`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0128eb68",
      "metadata": {
        "id": "0128eb68"
      },
      "outputs": [],
      "source": [
        "\n",
        "import whisper\n",
        "import pandas as pd\n",
        "\n",
        "def transcribe_spanish_audio(model, audio_path: Path):\n",
        "    \"\"\"\n",
        "    Transcribe a single Spanish audio file using Whisper.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : whisper.Whisper\n",
        "        Loaded Whisper model instance.\n",
        "    audio_path : Path\n",
        "        Path to the .wav file.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    text : str\n",
        "        Detected transcription text.\n",
        "    lang : str\n",
        "        Detected language code.\n",
        "    \"\"\"\n",
        "    result = model.transcribe(str(audio_path), language=\"es\", task=\"transcribe\", verbose=False)\n",
        "    return result[\"text\"].strip(), result.get(\"language\", \"unknown\")\n",
        "\n",
        "\n",
        "def translate_spanish_to_english(spanish_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Translate a Spanish transcription to English via OpenAI chat completion.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    spanish_text : str\n",
        "        Input Spanish text to translate.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    english_text : str\n",
        "        Translated English text.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"Translate the following Spanish medical transcription into clear, faithful English:\\n\\n\"\n",
        "        + spanish_text\n",
        "    )\n",
        "    result = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        temperature=0,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "    return result.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "def process_and_translate_audio(audio_folder: Path, output_csv: Path, model_size: str = \"base\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Run Whisper ASR on all .wav files in a folder, translate to English, and save results.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    audio_folder : Path\n",
        "        Directory containing .wav files.\n",
        "    output_csv : Path\n",
        "        Destination CSV for transcriptions and translations.\n",
        "    model_size : str, optional\n",
        "        Whisper model size (default: \"base\").\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame of results with columns:\n",
        "        [audio_file, spanish_transcription, english_translation, language_detected].\n",
        "    \"\"\"\n",
        "    model = whisper.load_model(model_size)\n",
        "    audio_files = sorted([f for f in os.listdir(audio_folder) if f.endswith(\".wav\")])\n",
        "\n",
        "    results = []\n",
        "    for fname in audio_files:\n",
        "        audio_path = audio_folder / fname\n",
        "        if not audio_path.exists():\n",
        "            continue\n",
        "        es_text, detected_lang = transcribe_spanish_audio(model, audio_path)\n",
        "        en_text = translate_spanish_to_english(es_text)\n",
        "        results.append({\n",
        "            \"audio_file\": fname,\n",
        "            \"spanish_transcription\": es_text,\n",
        "            \"english_translation\": en_text,\n",
        "            \"language_detected\": detected_lang\n",
        "        })\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
        "    print(\"Saved:\", output_csv)\n",
        "    return df\n",
        "\n",
        "# Example (do not auto-run):\n",
        "# trans_csv = LLM_OUT / \"audio_translations.csv\"\n",
        "# _ = process_and_translate_audio(AUDIO_DIR, trans_csv, model_size=\"base\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c531344",
      "metadata": {
        "id": "8c531344"
      },
      "source": [
        "\n",
        "## Step 2 â€“ ASR Evaluation (Input) <a id=\"asr-eval\"></a>\n",
        "This section computes WER and CER for the input ASR stage by aligning Whisper transcriptions with ground-truth text.\n",
        "\n",
        "**Assumptions**  \n",
        "- Ground truth is provided in `CSV_DIR / \"ground_truth.csv\"` with columns: `audio_file`, `ground_truth`.\n",
        "- Transcriptions are in `LLM_OUT / \"audio_translations.csv\"` with columns: `audio_file`, `spanish_transcription`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "24d16763",
      "metadata": {
        "id": "24d16763"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from jiwer import wer, cer\n",
        "\n",
        "def evaluate_asr_performance(ground_truth_csv: Path, trans_csv: Path, save_csv: Path) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compute WER and CER for input ASR against ground truth.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ground_truth_csv : Path\n",
        "        CSV file with columns: [audio_file, ground_truth].\n",
        "    trans_csv : Path\n",
        "        CSV file with columns: [audio_file, spanish_transcription].\n",
        "    save_csv : Path\n",
        "        Destination CSV for ASR metrics.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Evaluation results with per-file WER/CER.\n",
        "    \"\"\"\n",
        "    gt_df = pd.read_csv(ground_truth_csv)\n",
        "    tr_df = pd.read_csv(trans_csv)\n",
        "\n",
        "    # Defensive renaming for common variations.\n",
        "    gt_df = gt_df.rename(columns={\"filename\": \"audio_file\", \"spanish_text\": \"ground_truth\"})\n",
        "    tr_df = tr_df.rename(columns={\"transcription\": \"spanish_transcription\"})\n",
        "\n",
        "    df = pd.merge(tr_df[[\"audio_file\", \"spanish_transcription\"]],\n",
        "                  gt_df[[\"audio_file\", \"ground_truth\"]],\n",
        "                  on=\"audio_file\", how=\"inner\")\n",
        "\n",
        "    df[\"WER\"] = [wer(ref, hyp) for ref, hyp in zip(df[\"ground_truth\"], df[\"spanish_transcription\"])]\n",
        "    df[\"CER\"] = [cer(ref, hyp) for ref, hyp in zip(df[\"ground_truth\"], df[\"spanish_transcription\"])]\n",
        "\n",
        "    df.to_csv(save_csv, index=False)\n",
        "    print(\"Saved:\", save_csv)\n",
        "    return df\n",
        "\n",
        "# Example (do not auto-run):\n",
        "# gt_csv  = CSV_DIR / \"ground_truth.csv\"\n",
        "# trans_csv = LLM_OUT / \"audio_translations.csv\"\n",
        "# asr_csv = EVAL_DIR / \"asr_metrics.csv\"\n",
        "# _ = evaluate_asr_performance(gt_csv, trans_csv, asr_csv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4477ecb",
      "metadata": {
        "id": "f4477ecb"
      },
      "source": [
        "\n",
        "## Step 3 â€“ LLM Question Answering <a id=\"qa\"></a>\n",
        "This section queries an LLM with English questions derived from the ASR+translation step and provides answers based on a tabular blood-pressure dataset.\n",
        "\n",
        "**Inputs**: A CSV file with synthetic blood-pressure records.  \n",
        "**Outputs**: English answers and associated computed fields.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "85f89584",
      "metadata": {
        "id": "85f89584"
      },
      "outputs": [],
      "source": [
        "\n",
        "def ask_gpt(question_en: str, csv_block: str) -> dict:\n",
        "    \"\"\"\n",
        "    Query the LLM with a question and a CSV context block.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    question_en : str\n",
        "        English question derived from Spanish transcription.\n",
        "    csv_block : str\n",
        "        CSV content as a single string for in-context grounding.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        A dictionary with keys:\n",
        "        - \"answer\": str (LLM's English answer)\n",
        "        - \"computed_fields\": dict (optional structured fields)\n",
        "    \"\"\"\n",
        "\n",
        "    system = \"\"\"\n",
        "You are a careful and detail-oriented data analyst.\n",
        "\n",
        "You are given a synthetic blood pressure dataset in CSV format. It contains readings for one individual over the last 30 consecutive days, with the following columns:\n",
        "\n",
        "- date\n",
        "- age\n",
        "- sex\n",
        "- systolic_mmHg\n",
        "- diastolic_mmHg\n",
        "\n",
        "Use only the data in the CSV to answer all questions, except when normal blood pressure ranges are requested â€” in those cases, you may use external references but must cite your source.\n",
        "\n",
        "---\n",
        "\n",
        "ğŸ§  Interpretation Guidelines:\n",
        "\n",
        "- \"Today\" refers to the most recent date in the dataset.\n",
        "- \"Yesterday\" means the most recent date before \"today\", based on available data.\n",
        "- \"Last week\" or \"over the last week\" means the **7 most recent consecutive records** available in the dataset, including \"today\".\n",
        "- \"Last month\" means the **30 most recent consecutive records**, including \"today\".\n",
        "- These periods are based strictly on the CSV row order, not actual calendar weeks or months.\n",
        "- If any dates are missing, still treat the last 7 (or 30) rows as the valid window.\n",
        "- When performing calculations, use every numeric value listed without omission.\n",
        "- Do not remove or adjust outliers unless explicitly instructed to.\n",
        "- Perform exact arithmetic using all numeric values shown.\n",
        "- Round only the final averages to one decimal place.\n",
        "- If the dataset contains missing or non-numeric entries, ignore them but mention that they were skipped.\n",
        "- If a question refers to a specific date or range not found in the dataset, clearly state that the data is unavailable.\n",
        "- Use conversational date formats like â€œOctober 12â€ or â€œOctober 12 to 15â€ â€” avoid numeric formats like â€œ10/12/2025â€.\n",
        "\n",
        "---\n",
        "\n",
        "ğŸ§® Arithmetic Rules:\n",
        "- Always calculate numeric averages explicitly by summing all listed values and dividing by the total count.\n",
        "- Keep full precision during the calculation and round only the final result to one decimal place.\n",
        "- Do not smooth or approximate extreme values.\n",
        "- If you round intermediate steps, repeat the calculation using full-precision numbers.\n",
        "\n",
        "ğŸ’¬ Answer Style:\n",
        "\n",
        "- Write in natural, conversational English.\n",
        "- Address the user directly using â€œyouâ€ (e.g., â€œYour blood pressure wasâ€¦â€).\n",
        "\n",
        "---\n",
        "\n",
        "âœ… Response Format:\n",
        "\n",
        "Return all answers in the following JSON format:\n",
        "\n",
        "{ \"answer\": \"<English answer>\", \"computed_fields\": { \"numeric values used\" } }\n",
        "\n",
        "---\n",
        "\n",
        "ğŸ“Œ Example Questions and Answers:\n",
        "\n",
        "These illustrate tone and structure only. Actual answers must be based on the CSV data.\n",
        "\n",
        "Q: What are my systolic_mmHg and diastolic_mmHg blood pressures today?\n",
        "A: Your systolic blood pressure was [xx] mm Hg and your diastolic pressure was [yy] mm Hg today.\n",
        "\n",
        "Q: What were the values over the last week?\n",
        "A: Over the last 7 days, your systolic pressure averaged [xx] mm Hg and your diastolic pressure averaged [yy] mm Hg.\n",
        "\n",
        "Q: What is the trend of my blood pressure?\n",
        "A: Your blood pressure has shown a gradual increase in systolic values over the last 30 days, while your diastolic readings have remained stable.\n",
        "\n",
        "Q: What are the normal ranges for a person like me?\n",
        "A: Based on your age ([age from dataset] years) and sex ([male/female]), typical blood pressure values are approximately [xx/yy] mm Hg, according to [name and link to the external source].\n",
        "\"\"\"\n",
        "\n",
        "    user = (\n",
        "        f\"CSV:\\n{csv_block}\\n\\n\"\n",
        "        f\"Question:\\n{question_en}\\n\\n\"\n",
        "        \"Please analyze and return the response strictly following the JSON format defined above.\"\n",
        "    )\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        temperature=0,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user},\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    answer_text = response.choices[0].message.content.strip()\n",
        "\n",
        "    # Ensure output is JSON-safe\n",
        "    try:\n",
        "        result = json.loads(answer_text)\n",
        "    except json.JSONDecodeError:\n",
        "        result = {\"answer\": answer_text, \"computed_fields\": {}}\n",
        "\n",
        "    return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b90c1a9b",
      "metadata": {
        "id": "b90c1a9b"
      },
      "source": [
        "\n",
        "## Step 4 â€“ Translation and TTS <a id=\"tts\"></a>\n",
        "This section back-translates the LLM's English answers to Spanish and generates Spanish audio (TTS).\n",
        "\n",
        "Note: A simple gTTS-based fallback is provided (exports `.wav` via pydub). If your project includes a custom TTS, replace the fallback with your implementation and keep the same function signature.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "589d5361",
      "metadata": {
        "id": "589d5361"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "\n",
        "def translate_to_spanish(text_en: str) -> str:\n",
        "    \"\"\"\n",
        "    Translate English text to Spanish using OpenAI chat completion.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text_en : str\n",
        "        English text to translate.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Spanish translation.\n",
        "    \"\"\"\n",
        "    prompt = \"Translate the following English medical answer into Spanish:\\n\\n\" + text_en\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        temperature=0,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "def text_to_speech_spanish(text_es: str, out_wav_path: Path):\n",
        "    \"\"\"\n",
        "    Generate Spanish speech from text.\n",
        "\n",
        "    This fallback uses gTTS + pydub to export a WAV file if a custom TTS is\n",
        "    not available. Replace this function with your project-specific TTS if needed.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text_es : str\n",
        "        Spanish text to synthesize.\n",
        "    out_wav_path : Path\n",
        "        Destination path for the WAV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from gtts import gTTS\n",
        "        from pydub import AudioSegment\n",
        "        tmp_mp3 = out_wav_path.with_suffix(\".mp3\")\n",
        "        gTTS(text_es, lang=\"es\").save(tmp_mp3)\n",
        "        # Convert to WAV\n",
        "        audio = AudioSegment.from_file(tmp_mp3, format=\"mp3\")\n",
        "        audio.export(out_wav_path, format=\"wav\")\n",
        "        os.remove(tmp_mp3)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"TTS fallback failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af0262a4",
      "metadata": {
        "id": "af0262a4"
      },
      "source": [
        "\n",
        "## Step 5 â€“ Whisper Evaluation of TTS <a id=\"tts-eval\"></a>\n",
        "This section re-transcribes the generated Spanish audio responses using Whisper (base) and evaluates intelligibility against the ground-truth Spanish answers using WER, CER, and SER.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "f48ecab5",
      "metadata": {
        "id": "f48ecab5"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re, unicodedata\n",
        "import Levenshtein\n",
        "from jiwer import process_words\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize text for fair ASR comparison.\n",
        "    - Lowercase\n",
        "    - Strip accents\n",
        "    - Remove punctuation\n",
        "    - Collapse extra whitespace\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def evaluate_output_asr_whisper(\n",
        "    tts_csv: Path,\n",
        "    output_csv: Path = None,\n",
        "    model_size: str = \"base\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Evaluate Spanish TTS audios using Whisper by computing WER, CER, and SER\n",
        "    against ground-truth Spanish answers.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    tts_csv : Path\n",
        "        CSV with columns: [spanish_answer, audio_answer_file].\n",
        "    output_csv : Path, optional\n",
        "        Destination CSV for ASR metrics (defaults to EVAL_DIR / \"output_asr_metrics_whisper.csv\").\n",
        "    model_size : str, optional\n",
        "        Whisper model size (default: \"base\").\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame with per-file metrics.\n",
        "    \"\"\"\n",
        "    if output_csv is None:\n",
        "        output_csv = EVAL_DIR / \"output_asr_metrics_whisper.csv\"\n",
        "\n",
        "    if not tts_csv.exists():\n",
        "        raise FileNotFoundError(f\"Missing final results CSV: {tts_csv}\")\n",
        "\n",
        "    model = whisper.load_model(model_size)\n",
        "\n",
        "    df = pd.read_csv(tts_csv)\n",
        "    rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        gt = str(row.get(\"spanish_answer\", \"\")).strip()\n",
        "        audio_file = str(row.get(\"audio_answer_file\", \"\")).strip()\n",
        "        if not gt or not audio_file or not os.path.exists(audio_file):\n",
        "            continue\n",
        "\n",
        "        # Transcribe generated audio and normalize\n",
        "        res = model.transcribe(audio_file, language=\"es\", task=\"transcribe\", verbose=False)\n",
        "        hyp = res.get(\"text\", \"\").strip()\n",
        "\n",
        "        gt_norm = normalize_text(gt)\n",
        "        hyp_norm = normalize_text(hyp)\n",
        "\n",
        "        measures = process_words(gt_norm, hyp_norm)\n",
        "        wer_score = round(measures.wer, 4)\n",
        "        subs, dels, ins = measures.substitutions, measures.deletions, measures.insertions\n",
        "        cer_score = round(Levenshtein.distance(gt_norm, hyp_norm) / max(len(gt_norm), 1), 4)\n",
        "        ser_score = 0 if gt_norm == hyp_norm else 1\n",
        "\n",
        "        rows.append({\n",
        "            \"audio_file\": os.path.basename(audio_file),\n",
        "            \"ground_truth\": gt,\n",
        "            \"whisper_transcription\": hyp,\n",
        "            \"WER\": wer_score,\n",
        "            \"Substitutions\": subs,\n",
        "            \"Deletions\": dels,\n",
        "            \"Insertions\": ins,\n",
        "            \"CER\": cer_score,\n",
        "            \"SER\": ser_score,\n",
        "        })\n",
        "\n",
        "    out_df = pd.DataFrame(rows)\n",
        "    out_df.to_csv(output_csv, index=False)\n",
        "    print(\"Saved:\", output_csv)\n",
        "    return out_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9911afe5",
      "metadata": {
        "id": "9911afe5"
      },
      "source": [
        "\n",
        "## Orchestration (Optional) <a id=\"orchestration\"></a>\n",
        "The following function orchestrates all steps in sequence. Each step can also be run individually.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "852067e6",
      "metadata": {
        "id": "852067e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "outputId": "16864fbb-f06b-482f-b658-5821641349c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1303/1303 [00:03<00:00, 334.91frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 458/458 [00:05<00:00, 80.36frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 477/477 [00:02<00:00, 159.58frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 427/427 [00:02<00:00, 157.21frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "  0%|          | 0/573 [00:01<?, ?frames/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-84373167.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# Example (do not auto-run):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mbp_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCSV_DIR\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"synthetic_bp_84_male.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_full_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbp_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAUDIO_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhisper_model_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-84373167.py\u001b[0m in \u001b[0;36mrun_full_pipeline\u001b[0;34m(csv_path, audio_folder, whisper_model_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Step 1: ASR + Translation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtrans_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLM_OUT\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"audio_translations.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_and_translate_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhisper_model_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Step 2: Evaluate input ASR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4244449127.py\u001b[0m in \u001b[0;36mprocess_and_translate_audio\u001b[0;34m(audio_folder, output_csv, model_size)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0maudio_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mes_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetected_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranscribe_spanish_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0men_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_spanish_to_english\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mes_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         results.append({\n",
            "\u001b[0;32m/tmp/ipython-input-4244449127.py\u001b[0m in \u001b[0;36mtranscribe_spanish_audio\u001b[0;34m(model, audio_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mDetected\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \"\"\"\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"es\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"transcribe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"language\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"unknown\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py\u001b[0m in \u001b[0;36mtranscribe\u001b[0;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, carry_initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0mdecode_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprompt_reset_since\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDecodingResult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_with_fallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_segment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py\u001b[0m in \u001b[0;36mdecode_with_fallback\u001b[0;34m(segment)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecodingOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mdecode_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mneeds_fallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/whisper/decoding.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(model, mel, options, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecodingTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msingle\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/whisper/decoding.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, mel)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0mn_audio\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m         \u001b[0maudio_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_audio_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# encoder forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m         \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_audio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/whisper/decoding.py\u001b[0m in \u001b[0;36m_get_audio_features\u001b[0;34m(self, mel)\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0maudio_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             \u001b[0maudio_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         if audio_features.dtype != (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/whisper/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/whisper/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, xa, mask, kv_cache)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mkv_cache\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     ):\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_ln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkv_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attn_ln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkv_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/whisper/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, xa, mask, kv_cache)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqkv_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     def qkv_attention(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/whisper/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         return F.linear(\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import json\n",
        "def run_full_pipeline(csv_path: Path, audio_folder: Path, whisper_model_size: str = \"base\"):\n",
        "    \"\"\"\n",
        "    Execute the full pipeline, from input ASR/translation to TTS evaluation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    csv_path : Path\n",
        "        Path to the synthetic blood-pressure CSV.\n",
        "    audio_folder : Path\n",
        "        Directory containing input Spanish .wav files.\n",
        "    whisper_model_size : str, optional\n",
        "        Whisper model size, default \"base\".\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Summary dictionary with output artifact locations.\n",
        "    \"\"\"\n",
        "    # Step 1: ASR + Translation\n",
        "    trans_csv = LLM_OUT / \"audio_translations.csv\"\n",
        "    _ = process_and_translate_audio(audio_folder, trans_csv, model_size=whisper_model_size)\n",
        "\n",
        "    # Step 2: Evaluate input ASR\n",
        "    gt_csv = CSV_DIR / \"ground_truth.csv\"\n",
        "    asr_csv = EVAL_DIR / \"asr_metrics.csv\"\n",
        "    _ = evaluate_asr_performance(gt_csv, trans_csv, asr_csv)\n",
        "\n",
        "    # Step 3: Load tabular data for LLM grounding\n",
        "    df_bp = pd.read_csv(csv_path)\n",
        "    csv_block = df_bp.to_csv(index=False)\n",
        "\n",
        "    # Step 4: Q&A + Spanish TTS\n",
        "    results = []\n",
        "    tr_df = pd.read_csv(trans_csv)\n",
        "    for i, row in tr_df.iterrows():\n",
        "        q_en = row[\"english_translation\"]\n",
        "        ans = ask_gpt(q_en, csv_block)\n",
        "        ans_en = ans.get(\"answer\", \"\").strip()\n",
        "        ans_es = translate_to_spanish(ans_en)\n",
        "\n",
        "        out_wav = TTS_DIR / f\"answer_{i+1}_es.wav\"\n",
        "        text_to_speech_spanish(ans_es, out_wav)\n",
        "\n",
        "        results.append({\n",
        "            \"question_number\": i + 1,\n",
        "            \"audio_file_in\": row[\"audio_file\"],\n",
        "            \"spanish_question\": row[\"spanish_transcription\"],\n",
        "            \"english_question\": q_en,\n",
        "            \"english_answer\": ans_en,\n",
        "            \"spanish_answer\": ans_es,\n",
        "            \"audio_answer_file\": str(out_wav),\n",
        "            \"computed_fields\": json.dumps(ans.get(\"computed_fields\", {}))\n",
        "        })\n",
        "\n",
        "    final_csv = LLM_OUT / \"final_pipeline_results.csv\"\n",
        "    pd.DataFrame(results).to_csv(final_csv, index=False)\n",
        "    print(\"Saved:\", final_csv)\n",
        "\n",
        "    # Step 5: Evaluate TTS intelligibility\n",
        "    output_asr_csv = EVAL_DIR / \"output_asr_metrics_whisper.csv\"\n",
        "    _ = evaluate_output_asr_whisper(final_csv, output_csv=output_asr_csv, model_size=whisper_model_size)\n",
        "\n",
        "    return {\n",
        "        \"transcriptions_csv\": str(trans_csv),\n",
        "        \"input_asr_metrics_csv\": str(asr_csv),\n",
        "        \"final_pipeline_csv\": str(final_csv),\n",
        "        \"output_asr_metrics_csv\": str(output_asr_csv),\n",
        "    }\n",
        "\n",
        "# Example (do not auto-run):\n",
        "bp_csv = CSV_DIR / \"synthetic_bp_84_male.csv\"\n",
        "_ = run_full_pipeline(bp_csv, AUDIO_DIR, whisper_model_size=\"base\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b1fc405",
      "metadata": {
        "id": "5b1fc405"
      },
      "source": [
        "\n",
        "## Step 6 â€“ Results Summary <a id=\"summary\"></a>\n",
        "This section summarizes average WER, CER, and SER across input ASR and TTS evaluation outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "23c1ed41",
      "metadata": {
        "id": "23c1ed41"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "def summarize_results(input_asr_csv: Path, output_asr_csv: Path):\n",
        "    \"\"\"\n",
        "    Print dataset-level average metrics for input ASR and TTS ASR evaluation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_asr_csv : Path\n",
        "        CSV with per-file input ASR metrics.\n",
        "    output_asr_csv : Path\n",
        "        CSV with per-file TTS ASR metrics.\n",
        "    \"\"\"\n",
        "    print(\"Input ASR metrics (WER, CER):\")\n",
        "    if Path(input_asr_csv).exists():\n",
        "        d1 = pd.read_csv(input_asr_csv)\n",
        "        print(d1[[\"WER\", \"CER\", \"SER\"]].mean(numeric_only=True).to_frame(\"Average\"))\n",
        "    else:\n",
        "        print(\"Missing:\", input_asr_csv)\n",
        "\n",
        "    print(\"\\nTTS ASR metrics (WER, CER, SER):\")\n",
        "    if Path(output_asr_csv).exists():\n",
        "        d2 = pd.read_csv(output_asr_csv)\n",
        "        print(d2[[\"WER\", \"CER\", \"SER\"]].mean(numeric_only=True).to_frame(\"Average\"))\n",
        "    else:\n",
        "        print(\"Missing:\", output_asr_csv)\n",
        "\n",
        "# Example (do not auto-run):\n",
        "# summarize_results(EVAL_DIR / \"asr_metrics.csv\", EVAL_DIR / \"output_asr_metrics_whisper.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "summarize_results(EVAL_DIR / \"asr_metrics.csv\", EVAL_DIR / \"output_asr_metrics_whisper.csv\")"
      ],
      "metadata": {
        "id": "1jXT7dn1Nz8D",
        "outputId": "7704d700-9e0c-41a6-ff15-71acccf9db5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1jXT7dn1Nz8D",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input ASR metrics (WER, CER):\n",
            "      Average\n",
            "WER  0.035183\n",
            "CER  0.005517\n",
            "SER  0.333333\n",
            "\n",
            "TTS ASR metrics (WER, CER, SER):\n",
            "      Average\n",
            "WER  0.211000\n",
            "CER  0.160367\n",
            "SER  1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c776ee9f",
      "metadata": {
        "id": "c776ee9f"
      },
      "source": [
        "\n",
        "## Appendix <a id=\"appendix\"></a>\n",
        "- All paths are centralized under `PROJECT_ROOT` for reproducibility.  \n",
        "- Replace the TTS fallback with a project-specific implementation if available.  \n",
        "- The Whisper model size can be adjusted by changing `whisper_model_size` in the orchestration call.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}