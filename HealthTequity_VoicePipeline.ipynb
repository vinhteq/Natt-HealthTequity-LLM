{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nattaran/HealthTequity-LLM/blob/main/HealthTequity_VoicePipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü©∫ HealthTequity Voice Processing Pipeline\n",
        "\n",
        "## üìò Introduction\n",
        "This notebook demonstrates an end-to-end **Voice-to-Insight** pipeline developed for the **HealthTequity Case Study**.  \n",
        "It processes Spanish medical speech into actionable insights through **transcription**, **translation**, **LLM-based reasoning**, and **automated evaluation** using **WER**, **CER**, and **SER** metrics.  \n",
        "\n",
        "The system connects **speech understanding (ASR)** with **data analytics (LLM)** and **speech synthesis (TTS)**, forming a reproducible and modular workflow for healthcare data analysis.\n",
        "\n",
        "---\n",
        "\n",
        "### üß© Companion Notebooks\n",
        "Two supporting notebooks generate the datasets used in this pipeline:\n",
        "\n",
        "| Notebook | Purpose | Output Folder |\n",
        "|-----------|----------|---------------|\n",
        "| **Synthetic Blood Pressure Generator** | Creates a 30-day synthetic blood pressure dataset | `data/synthetic_csv/` |\n",
        "| **Spanish Audio Generator** | Produces Spanish-language health questions from the dataset | `data/Spanish_audio/` |\n",
        "\n",
        "> üîπ *Reviewers may also upload their own CSVs and audio recordings.*  \n",
        "> To compute **WER/CER/SER**, ensure a `ground_truth.csv` file exists with reference text for each input audio file.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è Pipeline Overview\n",
        "\n",
        "| Step | Component | Description | Output |\n",
        "|------|------------|-------------|---------|\n",
        "| **1Ô∏è‚É£** | **ASR (Whisper)** | Transcribes Spanish medical audio into text | Spanish transcript |\n",
        "| **2Ô∏è‚É£** | **Translation (GPT)** | Translates Spanish ‚Üí English for question understanding | `audio_translations.csv` |\n",
        "| **3Ô∏è‚É£** | **ASR Evaluation (Input)** | Compares transcriptions to ground truth ‚Üí computes **WER**, **CER**, **SER** | `input_asr_metrics.csv` |\n",
        "| **4Ô∏è‚É£** | **LLM Analysis (GPT-4o-mini)** | Answers English questions using the blood pressure CSV context | `final_pipeline_results.csv` |\n",
        "| **5Ô∏è‚É£** | **Spanish TTS (gTTS)** | Converts English answers back into spoken Spanish | `tts_audio/*.wav` |\n",
        "| **6Ô∏è‚É£** | **ASR Evaluation (Output)** | Evaluates TTS intelligibility via Whisper ASR (WER/CER/SER) | `output_asr_metrics.csv` |\n",
        "| **7Ô∏è‚É£** | **Visualization** | Plots a bar chart comparing input vs. output ASR metrics | `asr_comparison_chart.png` |\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Translation Toggle\n",
        "\n",
        "This pipeline can use either **Whisper** or **GPT** for Spanish-to-English translation:\n",
        "\n",
        "| Mode | How to Enable | Description |\n",
        "|------|----------------|-------------|\n",
        "| üß© **GPT Translation (Recommended)** | `USE_GPT_TRANSLATION = True` | Uses GPT-4o-mini for domain-accurate translations (preserves medical terms and numbers). Requires OpenAI API key. |\n",
        "| ‚öôÔ∏è **Whisper-Only Mode (Free)** | `USE_GPT_TRANSLATION = False` | Uses Whisper‚Äôs built-in `task=\"translate\"` mode. Works offline but may reduce translation accuracy. |\n",
        "\n",
        "You can change this toggle at the top of the ASR module.\n",
        "\n",
        "---\n",
        "\n",
        "### üìä Evaluation Metrics\n",
        "\n",
        "| Metric | Description | Interpretation |\n",
        "|---------|-------------|----------------|\n",
        "| **WER (Word Error Rate)** | Measures overall transcription accuracy (substitutions, deletions, insertions) | Lower = better |\n",
        "| **CER (Character Error Rate)** | Captures fine-grained textual differences | Sensitive to short utterances |\n",
        "| **SER (Sentence Error Rate)** | Binary metric ‚Äî whether a sentence matches exactly | 0 = perfect, 1 = any error |\n",
        "\n",
        "All metrics are computed for both **input** (Spanish audio ‚Üí text) and **output** (Spanish TTS ‚Üí text) stages.\n",
        "\n",
        "---\n",
        "\n",
        "### üß∞ System Notes\n",
        "- Default Whisper model is `\"base\"` but can be changed to `\"small\"`, `\"medium\"`, or `\"large\"`.  \n",
        "- All directories (`data/`, `results/`, etc.) are auto-created by the pipeline.  \n",
        "- Results and charts are saved under `/results/` for easy retrieval.  \n",
        "- The notebook can run entirely on Google Colab; only GPT translation requires an OpenAI API key.\n",
        "\n",
        "---\n",
        "\n",
        "### üìÇ Folder Structure\n",
        "\n",
        "```text\n",
        "/content/drive/MyDrive/HealthTequity-LLM/\n",
        "‚îÇ\n",
        "‚îú‚îÄ‚îÄ data/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ synthetic_csv/              ‚Üê Synthetic BP data + ground truth\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ synthetic_bp_one_person.csv\n",
        "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ground_truth.csv\n",
        "‚îÇ   ‚îÇ\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ Spanish_audio/              ‚Üê Input Spanish audio files\n",
        "‚îÇ       ‚îú‚îÄ‚îÄ question_1_es.wav\n",
        "‚îÇ       ‚îú‚îÄ‚îÄ question_2_es.wav\n",
        "‚îÇ       ‚îî‚îÄ‚îÄ ...\n",
        "‚îÇ\n",
        "‚îú‚îÄ‚îÄ results/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ llm_outputs/                ‚Üê Translations + LLM responses\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ tts_audio/                  ‚Üê Spanish audio answers\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ evaluation_metrics/         ‚Üê WER/CER/SER + chart\n",
        "‚îÇ\n",
        "‚îú‚îÄ‚îÄ BloodPressure_Generator.ipynb\n",
        "‚îú‚îÄ‚îÄ SpanishAudio_Generator.ipynb\n",
        "‚îî‚îÄ‚îÄ HealthTequity_VoicePipeline.ipynb\n"
      ],
      "metadata": {
        "id": "XBAdMOleww-o"
      },
      "id": "XBAdMOleww-o"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÅ Step 1 ‚Äî Mount Google Drive and Sync Project Repository\n",
        "\n",
        "This step connects your Colab environment to Google Drive and ensures you have the latest version of the **HealthTequity-LLM** project.\n",
        "\n",
        "**What this does:**\n",
        "1. Clears any previously mounted Drive (prevents the ‚Äúmountpoint already contains files‚Äù error).\n",
        "2. Mounts Google Drive at `/content/drive`.\n",
        "3. Clones the `HealthTequity-LLM` GitHub repository into your Drive if it doesn‚Äôt exist,  \n",
        "   or updates it with the latest version if it already exists.\n",
        "\n",
        "**After this cell runs successfully:**\n",
        "- You‚Äôll be working directly from  \n",
        "  `/content/drive/MyDrive/HealthTequity-LLM`\n",
        "- All project data, code, and outputs will stay persistent in your Google Drive.\n"
      ],
      "metadata": {
        "id": "0pkaokMr0f7Q"
      },
      "id": "0pkaokMr0f7Q"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# üìÅ STEP 1 ‚Äî Mount Google Drive and Clone/Update Repository\n",
        "# ==========================================================\n",
        "from google.colab import drive\n",
        "import os, shutil\n",
        "\n",
        "MOUNT_POINT = '/content/drive'\n",
        "REPO_URL = \"https://github.com/nattaran/HealthTequity-LLM.git\"\n",
        "REPO_PATH = f\"{MOUNT_POINT}/MyDrive/HealthTequity-LLM\"\n",
        "\n",
        "# --- Clean any existing mountpoint to prevent ValueError ---\n",
        "if os.path.exists(MOUNT_POINT) and os.path.isdir(MOUNT_POINT) and os.listdir(MOUNT_POINT):\n",
        "    print(f\"‚öôÔ∏è Clearing existing mountpoint: {MOUNT_POINT}\")\n",
        "    try:\n",
        "        shutil.rmtree(MOUNT_POINT)\n",
        "        os.makedirs(MOUNT_POINT)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Warning: Could not fully clear mountpoint: {e}\")\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "print(\"üîó Mounting Google Drive...\")\n",
        "drive.mount(MOUNT_POINT, force_remount=True)\n",
        "\n",
        "# --- Clone or update GitHub repo ---\n",
        "if not os.path.exists(REPO_PATH):\n",
        "    print(f\"üì¶ Cloning repository into {REPO_PATH}...\")\n",
        "    !git clone {REPO_URL} {REPO_PATH}\n",
        "else:\n",
        "    print(\"üîÑ Repository already exists ‚Äî updating...\")\n",
        "    %cd {REPO_PATH}\n",
        "    !git fetch origin\n",
        "    !git pull\n",
        "\n",
        "%cd {REPO_PATH}\n",
        "print(f\"‚úÖ Environment ready. Working directory: {os.getcwd()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9GGDiUrHnS2",
        "outputId": "aef57360-f19f-4b16-c710-5150f9b5e1bb"
      },
      "id": "R9GGDiUrHnS2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîó Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "üîÑ Repository already exists ‚Äî updating...\n",
            "/content/drive/MyDrive/HealthTequity-LLM\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (3/3), 12.41 KiB | 181.00 KiB/s, done.\n",
            "From https://github.com/nattaran/HealthTequity-LLM\n",
            "   645c368..0dd138f  main       -> origin/main\n",
            "Updating 645c368..0dd138f\n",
            "Fast-forward\n",
            " HealthTequity_VoicePipeline.ipynb | 1354 \u001b[32m+++++++++++++++++++++++\u001b[m\u001b[31m--------------\u001b[m\n",
            " 1 file changed, 837 insertions(+), 517 deletions(-)\n",
            "/content/drive/MyDrive/HealthTequity-LLM\n",
            "‚úÖ Environment ready. Working directory: /content/drive/MyDrive/HealthTequity-LLM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öôÔ∏è Step 2 ‚Äî Install Project Dependencies\n",
        "\n",
        "This step installs all the required Python packages for the HealthTequity-LLM pipeline.\n",
        "\n",
        "**Notes:**\n",
        "- Run this cell once per Colab session.\n",
        "- Dependencies are listed in `requirements.txt` at the repository root.\n",
        "- You can add or pin package versions there (e.g., `whisper==1.0`, `jiwer==3.0.2`).\n",
        "- Colab may show warnings for already-installed packages ‚Äî you can safely ignore them.\n"
      ],
      "metadata": {
        "id": "6wKfk2K51qpt"
      },
      "id": "6wKfk2K51qpt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a53aba9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a53aba9f",
        "outputId": "c2034e65-43ac-4ba6-c3ce-e76fd9f7ad04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git (from -r requirements.txt (line 22))\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-w3s6_5tk\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-w3s6_5tk\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (2.2.2)\n",
            "Requirement already satisfied: matplotlib>=3.8.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (3.10.0)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (2.8.0+cu126)\n",
            "Requirement already satisfied: openai>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 15)) (1.109.1)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 16)) (0.12.0)\n",
            "Collecting whisper (from -r requirements.txt (line 20))\n",
            "  Downloading whisper-1.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ffmpeg-python>=0.2.0 (from -r requirements.txt (line 25))\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 26)) (0.13.1)\n",
            "Requirement already satisfied: librosa>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 27)) (0.11.0)\n",
            "Requirement already satisfied: pydub>=0.25.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 28)) (0.25.1)\n",
            "Collecting jiwer>=3.0.3 (from -r requirements.txt (line 31))\n",
            "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting python-Levenshtein>=0.25.0 (from -r requirements.txt (line 32))\n",
            "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting gtts>=2.3.2 (from -r requirements.txt (line 35))\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting deep-translator>=1.11.4 (from -r requirements.txt (line 36))\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: python-dotenv>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 39)) (1.1.1)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 40)) (4.67.1)\n",
            "Collecting vosk==0.3.45 (from -r requirements.txt (line 43))\n",
            "  Downloading vosk-0.3.45-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from vosk==0.3.45->-r requirements.txt (line 43)) (2.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vosk==0.3.45->-r requirements.txt (line 43)) (2.32.4)\n",
            "Collecting srt (from vosk==0.3.45->-r requirements.txt (line 43))\n",
            "  Downloading srt-3.5.3.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.12/dist-packages (from vosk==0.3.45->-r requirements.txt (line 43)) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->-r requirements.txt (line 7)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->-r requirements.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->-r requirements.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (3.2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (3.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (1.3.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.6.0->-r requirements.txt (line 16)) (2024.11.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from whisper->-r requirements.txt (line 20)) (1.17.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625->-r requirements.txt (line 22)) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625->-r requirements.txt (line 22)) (0.60.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from ffmpeg-python>=0.2.0->-r requirements.txt (line 25)) (1.0.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.16.2)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.0.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.1.2)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer>=3.0.3->-r requirements.txt (line 31)) (8.3.0)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer>=3.0.3->-r requirements.txt (line 31))\n",
            "  Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Collecting Levenshtein==0.27.1 (from python-Levenshtein>=0.25.0->-r requirements.txt (line 32))\n",
            "  Downloading levenshtein-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting click>=8.1.8 (from jiwer>=3.0.3->-r requirements.txt (line 31))\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.12/dist-packages (from deep-translator>=1.11.4->-r requirements.txt (line 36)) (4.13.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai>=1.12.0->-r requirements.txt (line 15)) (3.11)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator>=1.11.4->-r requirements.txt (line 36)) (2.8)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->vosk==0.3.45->-r requirements.txt (line 43)) (2.23)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=1.12.0->-r requirements.txt (line 15)) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=1.12.0->-r requirements.txt (line 15)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.12.0->-r requirements.txt (line 15)) (0.16.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper==20250625->-r requirements.txt (line 22)) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa>=0.10.1->-r requirements.txt (line 27)) (4.5.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.12.0->-r requirements.txt (line 15)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.12.0->-r requirements.txt (line 15)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.12.0->-r requirements.txt (line 15)) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vosk==0.3.45->-r requirements.txt (line 43)) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vosk==0.3.45->-r requirements.txt (line 43)) (2.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa>=0.10.1->-r requirements.txt (line 27)) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.0->-r requirements.txt (line 11)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.0->-r requirements.txt (line 11)) (3.0.3)\n",
            "Downloading vosk-0.3.45-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
            "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: whisper, openai-whisper, srt\n",
            "  Building wheel for whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for whisper: filename=whisper-1.1.10-py3-none-any.whl size=41120 sha256=844b30148565635d971f7d1150861477089a2c6dbf052350f723390c3d80160f\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/b8/4e/9c4c3351d670e06746a340fb4b7d854c76517eec225e5b32b1\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=637c23df617fa0c27d36abaec3860a68d685d234f2dfe58b4f5bd0ed7d0e054a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8pwcn7ll/wheels/c3/03/25/5e0ba78bc27a3a089f137c9f1d92fdfce16d06996c071a016c\n",
            "  Building wheel for srt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for srt: filename=srt-3.5.3-py3-none-any.whl size=22427 sha256=43e155a24de5ad725b4b27f22421ad899e1dd79179c9f9fab0a60236be90a8ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/75/5b/e1d5c3756631e4bda806f6cc9640153b39484bb6f7b0b8def3\n",
            "Successfully built whisper openai-whisper srt\n",
            "Installing collected packages: whisper, srt, rapidfuzz, ffmpeg-python, click, vosk, Levenshtein, jiwer, gtts, deep-translator, python-Levenshtein, openai-whisper\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.3.0\n",
            "    Uninstalling click-8.3.0:\n",
            "      Successfully uninstalled click-8.3.0\n",
            "Successfully installed Levenshtein-0.27.1 click-8.1.8 deep-translator-1.11.4 ffmpeg-python-0.2.0 gtts-2.5.4 jiwer-4.0.0 openai-whisper-20250625 python-Levenshtein-0.27.1 rapidfuzz-3.14.1 srt-3.5.3 vosk-0.3.45 whisper-1.1.10\n",
            "\n",
            "‚úÖ Package installation complete.\n",
            "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "geopandas                                1.1.1\n",
            "jiwer                                    4.0.0\n",
            "matplotlib                               3.10.0\n",
            "matplotlib-inline                        0.1.7\n",
            "matplotlib-venn                          1.1.2\n",
            "openai                                   1.109.1\n",
            "openai-whisper                           20250625\n",
            "pandas                                   2.2.2\n",
            "pandas-datareader                        0.10.0\n",
            "pandas-gbq                               0.29.2\n",
            "pandas-stubs                             2.2.2.240909\n",
            "sklearn-pandas                           2.2.0\n",
            "torch                                    2.8.0+cu126\n",
            "torchao                                  0.10.0\n",
            "torchaudio                               2.8.0+cu126\n",
            "torchdata                                0.11.0\n",
            "torchsummary                             1.5.1\n",
            "torchtune                                0.6.1\n",
            "torchvision                              0.23.0+cu126\n",
            "whisper                                  1.1.10\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install project requirements (run once per session)\n",
        "# If you prefer to pin versions, ensure requirements.txt includes exact versions.\n",
        "!pip install -r requirements.txt\n",
        "# Verify installation of key modules\n",
        "import sys\n",
        "print(\"\\n‚úÖ Package installation complete.\")\n",
        "print(\"Python version:\", sys.version)\n",
        "!pip list | grep -E \"openai|whisper|jiwer|pandas|torch|matplotlib\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÇ Step 3 ‚Äî Project Paths and Data Dependencies\n",
        "\n",
        "This step defines the directory structure used by the **HealthTequity Voice Pipeline**  \n",
        "and ensures all necessary folders exist within your Google Drive.\n",
        "\n",
        "By default, the **Audio Generation Notebook** automatically produces:\n",
        "- Spanish question audio files (`.wav`) under `data/Spanish_audio/`\n",
        "- The corresponding `ground_truth.csv` file under `data/synthetic_csv/`\n",
        "- A synthetic blood-pressure dataset (`synthetic_bp_one_person.csv`) under the same folder\n",
        "\n",
        "These files are automatically detected when this pipeline runs.\n",
        "\n",
        "---\n",
        "\n",
        "### üß© Reviewer Flexibility\n",
        "If you prefer to **test with your own data**, you can:\n",
        "- Upload custom `.wav` files to `data/Spanish_audio/`\n",
        "- Upload your own ground truth file to `data/synthetic_csv/ground_truth.csv`\n",
        "- Upload a new blood pressure dataset to `data/synthetic_csv/`\n",
        "\n",
        "Your files will be automatically used by the pipeline ‚Äî no code modification needed.\n",
        "\n",
        "---\n",
        "\n",
        "### üóÇ Folder Overview\n",
        "\n",
        "| Folder | Purpose |\n",
        "|---------|----------|\n",
        "| `data/synthetic_csv/` | Synthetic or user-provided CSV datasets and ground truth |\n",
        "| `data/Spanish_audio/` | Input Spanish `.wav` question audio files |\n",
        "| `results/llm_outputs/` | LLM-generated question‚Äìanswer CSV outputs |\n",
        "| `results/evaluation_metrics/` | ASR evaluation metrics (WER, CER, SER) |\n",
        "| `results/tts_audio/` | Generated Spanish audio (TTS) responses |\n",
        "\n",
        "All paths are created automatically under:\n",
        "`/content/drive/MyDrive/HealthTequity-LLM`\n"
      ],
      "metadata": {
        "id": "3o7I6l8g-sXa"
      },
      "id": "3o7I6l8g-sXa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5ec22c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5ec22c8",
        "outputId": "3d384353-6904-417e-c9a8-36499d638398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Directory structure verified:\n",
            "üìÅ Project root:        /content/drive/MyDrive/HealthTequity-LLM\n",
            "üìÑ Ground truth file:   ‚úÖ Found\n",
            "üéß Spanish audio files: 9 found\n",
            "üìä Blood pressure CSV:  ‚úÖ Found\n",
            "üìä Results directory:   /content/drive/MyDrive/HealthTequity-LLM/results\n",
            "üß† LLM outputs:         /content/drive/MyDrive/HealthTequity-LLM/results/llm_outputs\n",
            "üßÆ Evaluation metrics:  /content/drive/MyDrive/HealthTequity-LLM/results/evaluation_metrics\n",
            "üîâ TTS audio:           /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio\n"
          ]
        }
      ],
      "source": [
        "# ==========================================================\n",
        "# üìÇ STEP 3 ‚Äî Project Paths and Data Dependencies\n",
        "# ==========================================================\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# --- Define project root in Google Drive ---\n",
        "PROJECT_ROOT = Path(\"/content/drive/MyDrive/HealthTequity-LLM\")\n",
        "\n",
        "# --- Define subdirectories ---\n",
        "DATA_DIR    = PROJECT_ROOT / \"data\"\n",
        "CSV_DIR     = DATA_DIR / \"synthetic_csv\"\n",
        "AUDIO_DIR   = DATA_DIR / \"Spanish_audio\"\n",
        "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
        "LLM_OUT     = RESULTS_DIR / \"llm_outputs\"\n",
        "EVAL_DIR    = RESULTS_DIR / \"evaluation_metrics\"\n",
        "TTS_DIR     = RESULTS_DIR / \"tts_audio\"\n",
        "\n",
        "# --- Create folders if missing (safe & repeatable) ---\n",
        "for p in [DATA_DIR, CSV_DIR, AUDIO_DIR, RESULTS_DIR, LLM_OUT, EVAL_DIR, TTS_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- Verify generated or uploaded data ---\n",
        "ground_truth = CSV_DIR / \"ground_truth.csv\"\n",
        "audio_files = list(AUDIO_DIR.glob(\"*.wav\"))\n",
        "bp_csv = CSV_DIR / \"synthetic_bp_one_person.csv\"\n",
        "\n",
        "print(\"‚úÖ Directory structure verified:\")\n",
        "print(f\"üìÅ Project root:        {PROJECT_ROOT}\")\n",
        "print(f\"üìÑ Ground truth file:   {'‚úÖ Found' if ground_truth.exists() else '‚ö†Ô∏è Missing'}\")\n",
        "print(f\"üéß Spanish audio files: {len(audio_files)} found\")\n",
        "print(f\"üìä Blood pressure CSV:  {'‚úÖ Found' if bp_csv.exists() else '‚ö†Ô∏è Missing'}\")\n",
        "print(f\"üìä Results directory:   {RESULTS_DIR}\")\n",
        "print(f\"üß† LLM outputs:         {LLM_OUT}\")\n",
        "print(f\"üßÆ Evaluation metrics:  {EVAL_DIR}\")\n",
        "print(f\"üîâ TTS audio:           {TTS_DIR}\")\n",
        "\n",
        "if not ground_truth.exists() or not audio_files:\n",
        "    print(\"\\n‚ö†Ô∏è Note: Run the Audio Generation Notebook or upload your own data to the above folders.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîë Step 4 ‚Äî OpenAI API Key Initialization\n",
        "\n",
        "This step initializes the OpenAI client for all LLM operations (e.g., translations, question answering).\n",
        "\n",
        "**How it works:**\n",
        "- The code first looks for your API key in the Colab environment (`os.environ[\"OPENAI_API_KEY\"]`).\n",
        "- If no key is found, you‚Äôll be securely prompted to paste it (input remains hidden).\n",
        "- Once entered, the key is stored in the current runtime session for later API calls.\n",
        "\n",
        "> üí° **Security note:**  \n",
        "> Your key is **not saved permanently** ‚Äî it will reset when the Colab runtime restarts.  \n",
        "> Reviewers can safely run this section with their own OpenAI API keys.\n"
      ],
      "metadata": {
        "id": "dwCedjfe_c-S"
      },
      "id": "dwCedjfe_c-S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2147149",
      "metadata": {
        "id": "d2147149",
        "outputId": "d15e54a7-c7cf-4838-de05-2a3cc06321e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è OpenAI API key not found in environment.\n",
            "üîê Paste your OpenAI API key (input hidden): ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "‚úÖ OpenAI client initialized successfully.\n"
          ]
        }
      ],
      "source": [
        "# ==========================================================\n",
        "# üîë STEP 4 ‚Äî OpenAI API Key Initialization\n",
        "# ==========================================================\n",
        "import os\n",
        "from getpass import getpass\n",
        "from openai import OpenAI\n",
        "\n",
        "# --- Check if API key exists; if not, prompt user securely ---\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    print(\"‚ö†Ô∏è OpenAI API key not found in environment.\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"üîê Paste your OpenAI API key (input hidden): \").strip()\n",
        "else:\n",
        "    print(\"‚úÖ Found existing OpenAI API key in environment.\")\n",
        "\n",
        "# --- Initialize client (raises error if key invalid) ---\n",
        "try:\n",
        "    client = OpenAI()\n",
        "    print(\"‚úÖ OpenAI client initialized successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to initialize OpenAI client: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üó£Ô∏è Step 5 ‚Äî ASR (Whisper) and Spanish ‚Üí English Translation\n",
        "\n",
        "This section runs the **Automatic Speech Recognition (ASR)** and **translation** stage of the pipeline.  \n",
        "It is used on both sides of the workflow:\n",
        "\n",
        "1. **Input side** ‚Äì transcribes Spanish question audio and translates it to English for LLM analysis.  \n",
        "2. **Output side** ‚Äì re-transcribes generated Spanish TTS responses for ASR evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è How It Works\n",
        "- **Whisper ASR** performs transcription directly from audio.  \n",
        "- **Whisper Translation Mode** (`task=\"translate\"`) produces English text automatically ‚Äî no API key required.  \n",
        "- Optionally, reviewers can enable **OpenAI GPT translation** for more fluent medical phrasing.\n",
        "\n",
        "---\n",
        "\n",
        "### üîß Configuration\n",
        "You can control translation behavior with this flag:\n",
        "```python\n",
        "USE_GPT_TRANSLATION = True  # Set True to use GPT-based translation instead of Whisper\n"
      ],
      "metadata": {
        "id": "Y4vB58i5CTN_"
      },
      "id": "Y4vB58i5CTN_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0128eb68",
      "metadata": {
        "id": "0128eb68"
      },
      "outputs": [],
      "source": [
        "# ==========================================================\n",
        "# üó£Ô∏è STEP 5 ‚Äî ASR (Whisper) and Spanish ‚Üí English Translation\n",
        "# ==========================================================\n",
        "import whisper\n",
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Toggle for optional GPT translation ---\n",
        "USE_GPT_TRANSLATION = True   # Default: Whisper only (free)\n",
        "WHISPER_MODEL_SIZE = \"base\"   # Adjustable model size (\"tiny\", \"small\", \"medium\", \"large\")\n",
        "\n",
        "def transcribe_spanish_audio(model, audio_path: Path):\n",
        "    \"\"\"\n",
        "    Transcribe a single Spanish audio file using Whisper.\n",
        "    Returns Spanish text and detected language.\n",
        "    \"\"\"\n",
        "    result = model.transcribe(str(audio_path), language=\"es\", task=\"transcribe\", verbose=False)\n",
        "    return result[\"text\"].strip(), result.get(\"language\", \"unknown\")\n",
        "\n",
        "def translate_audio_whisper(model, audio_path: Path):\n",
        "    \"\"\"\n",
        "    Use Whisper‚Äôs translation mode to directly translate Spanish ‚Üí English.\n",
        "    \"\"\"\n",
        "    result = model.transcribe(str(audio_path), task=\"translate\", verbose=False)\n",
        "    return result[\"text\"].strip()\n",
        "\n",
        "def translate_spanish_to_english_gpt(spanish_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Optional GPT translation for higher fluency (requires OpenAI API key).\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"Translate the following Spanish medical transcription into clear, faithful English:\\n\\n\"\n",
        "        + spanish_text\n",
        "    )\n",
        "    result = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        temperature=0,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "    return result.choices[0].message.content.strip()\n",
        "\n",
        "def process_and_translate_audio(audio_folder: Path, output_csv: Path,\n",
        "                                model_size: str = WHISPER_MODEL_SIZE) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Run Whisper ASR on all .wav files in a folder,\n",
        "    translate Spanish ‚Üí English (Whisper or GPT),\n",
        "    and save results to CSV.\n",
        "    \"\"\"\n",
        "    print(f\"üéß Loading Whisper model: {model_size}\")\n",
        "    model = whisper.load_model(model_size)\n",
        "\n",
        "    audio_files = sorted([f for f in os.listdir(audio_folder) if f.endswith(\".wav\")])\n",
        "    if not audio_files:\n",
        "        print(f\"‚ö†Ô∏è No audio files found in {audio_folder}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    results = []\n",
        "    print(f\"üîç Processing {len(audio_files)} audio files...\")\n",
        "\n",
        "    for fname in audio_files:\n",
        "        audio_path = audio_folder / fname\n",
        "        # 1Ô∏è‚É£ Spanish transcription\n",
        "        es_text, detected_lang = transcribe_spanish_audio(model, audio_path)\n",
        "        # 2Ô∏è‚É£ Translation (Whisper ‚Üí English or GPT optional)\n",
        "        if USE_GPT_TRANSLATION:\n",
        "            en_text = translate_spanish_to_english_gpt(es_text)\n",
        "        else:\n",
        "            en_text = translate_audio_whisper(model, audio_path)\n",
        "\n",
        "        results.append({\n",
        "            \"audio_file\": fname,\n",
        "            \"spanish_transcription\": es_text,\n",
        "            \"english_translation\": en_text,\n",
        "            \"language_detected\": detected_lang\n",
        "        })\n",
        "        print(f\"‚úÖ {fname} ‚Üí processed\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"\\nüíæ Results saved to: {output_csv}\")\n",
        "    return df\n",
        "\n",
        "# Example (do not auto-run):\n",
        "# trans_csv = LLM_OUT / \"audio_translations.csv\"\n",
        "# trans_df = process_and_translate_audio(AUDIO_DIR, trans_csv, model_size=\"base\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üßÆ Step 6 ‚Äî Unified ASR Evaluation (WER / CER / SER)\n",
        "\n",
        "This section defines a **single reusable evaluation module** for computing ASR performance metrics on any\n",
        "pair of *ground-truth* and *predicted* transcriptions.\n",
        "\n",
        "It is used in two contexts:\n",
        "1. **Input side evaluation** ‚Äì compares Whisper‚Äôs transcribed Spanish questions against the ground-truth CSV.  \n",
        "2. **Output side evaluation** ‚Äì compares Whisper‚Äôs re-transcription of TTS-generated Spanish responses\n",
        "   against the LLM-generated ground-truth Spanish text.\n",
        "\n",
        "---\n",
        "\n",
        "### üìè Metrics Computed\n",
        "| Metric | Description |\n",
        "|---------|--------------|\n",
        "| **WER (Word Error Rate)** | Fraction of words incorrectly predicted (insertions + deletions + substitutions) / total words |\n",
        "| **CER (Character Error Rate)** | Character-level edit distance normalized by text length |\n",
        "| **SER (Sentence Error Rate)** | Percentage of sentences that are not identical to ground truth |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è How to Use\n",
        "Call the function below with:\n",
        "```python\n",
        "evaluate_asr_whisper(\n",
        "    gt_csv=Path(...),          # CSV with ground_truth_text\n",
        "    audio_folder=Path(...),    # folder of .wav files to evaluate\n",
        "    output_csv=Path(...),      # where to save evaluation results\n",
        "    model_size=\"base\"          # whisper model size (default)\n",
        ")\n"
      ],
      "metadata": {
        "id": "B0hE4n4YDQpV"
      },
      "id": "B0hE4n4YDQpV"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gJMncazPoep-"
      },
      "id": "gJMncazPoep-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24d16763",
      "metadata": {
        "id": "24d16763"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ==========================================================\n",
        "# üß© STEP 8 ‚Äî UNIFIED ASR EVALUATION FUNCTION (FINAL)\n",
        "# ==========================================================\n",
        "import os\n",
        "import pandas as pd\n",
        "import whisper\n",
        "import re, unicodedata, Levenshtein\n",
        "from jiwer import process_words\n",
        "from pathlib import Path\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    \"\"\"Lowercase, strip accents, and remove punctuation.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', text)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def evaluate_asr_whisper(\n",
        "    gt_csv: Path,\n",
        "    audio_folder: Path,\n",
        "    output_csv: Path,\n",
        "    model_size: str = \"base\",\n",
        "    gt_text_col: str = \"ground_truth_text\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate ASR performance using Whisper and compute WER, CER, SER.\n",
        "    Works for both input and output sides of the pipeline.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    gt_csv : Path\n",
        "        CSV file containing at least columns [audio_file, ground_truth_text]\n",
        "    audio_folder : Path\n",
        "        Directory containing .wav audio files to evaluate\n",
        "    output_csv : Path\n",
        "        Output CSV for detailed ASR metrics\n",
        "    model_size : str, optional\n",
        "        Whisper model size to load (default: 'base')\n",
        "        Options: 'tiny', 'base', 'small', 'medium', 'large'\n",
        "    gt_text_col : str, optional\n",
        "        Column name for the reference text (default: 'ground_truth_text')\n",
        "    \"\"\"\n",
        "    if not Path(gt_csv).exists():\n",
        "        raise FileNotFoundError(f\"‚ùå Ground truth CSV not found at: {gt_csv}\")\n",
        "    if not Path(audio_folder).exists():\n",
        "        raise FileNotFoundError(f\"‚ùå Audio folder not found at: {audio_folder}\")\n",
        "\n",
        "    df_gt = pd.read_csv(gt_csv)\n",
        "    if \"audio_file\" not in df_gt.columns or gt_text_col not in df_gt.columns:\n",
        "        raise ValueError(f\"‚ùå CSV must contain ['audio_file', '{gt_text_col}'] columns.\")\n",
        "\n",
        "    print(f\"üéß Loading Whisper model: {model_size}\")\n",
        "    model = whisper.load_model(model_size)\n",
        "\n",
        "    results = []\n",
        "    print(f\"üîç Evaluating {len(df_gt)} audio files...\")\n",
        "\n",
        "    for _, row in df_gt.iterrows():\n",
        "        audio_name = row[\"audio_file\"]\n",
        "        gt_text = str(row[gt_text_col]).strip()\n",
        "        audio_path = Path(audio_folder) / audio_name\n",
        "        if not audio_path.exists():\n",
        "            print(f\"‚ö†Ô∏è Missing audio file: {audio_name}\")\n",
        "            continue\n",
        "\n",
        "        # --- Transcribe ---\n",
        "        result = model.transcribe(str(audio_path), language=\"es\", task=\"transcribe\", verbose=False)\n",
        "        hyp_text = result[\"text\"].strip()\n",
        "\n",
        "        # --- Normalize & compute metrics ---\n",
        "        gt_norm, hyp_norm = normalize_text(gt_text), normalize_text(hyp_text)\n",
        "        measures = process_words(gt_norm, hyp_norm)\n",
        "        wer = round(measures.wer, 4)\n",
        "        cer = round(Levenshtein.distance(gt_norm, hyp_norm) / max(len(gt_norm), 1), 4)\n",
        "        ser = 0 if gt_norm == hyp_norm else 1\n",
        "\n",
        "        results.append({\n",
        "            \"audio_file\": audio_name,\n",
        "            \"ground_truth\": gt_text,\n",
        "            \"whisper_transcription\": hyp_text,\n",
        "            \"WER\": wer,\n",
        "            \"CER\": cer,\n",
        "            \"SER\": ser,\n",
        "            \"Substitutions\": measures.substitutions,\n",
        "            \"Deletions\": measures.deletions,\n",
        "            \"Insertions\": measures.insertions\n",
        "        })\n",
        "        print(f\"‚úÖ {audio_name} ‚Üí WER={wer}, CER={cer}, SER={ser}\")\n",
        "\n",
        "    # --- Save results ---\n",
        "    out_df = pd.DataFrame(results)\n",
        "    out_df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"\\nüíæ ASR metrics saved to: {output_csv}\")\n",
        "    print(f\"üìä Average WER={out_df['WER'].mean():.3f}, CER={out_df['CER'].mean():.3f}, SER={out_df['SER'].mean():.3f}\")\n",
        "    return out_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Step 7 ‚Äî ASR Metrics Visualization (Input vs Output)\n",
        "\n",
        "This step visualizes the ASR performance of the pipeline for both the **input** and **output** stages.\n",
        "\n",
        "**Purpose:**\n",
        "- The **Input ASR** evaluates how well Whisper transcribes Spanish question audios compared to ground truth.  \n",
        "- The **Output ASR** evaluates Whisper‚Äôs accuracy in re-transcribing the TTS-generated Spanish answers.\n",
        "\n",
        "The plot shows average **WER (Word Error Rate)**, **CER (Character Error Rate)**, and **SER (Sentence Error Rate)** side-by-side.\n",
        "\n",
        "**Expected Input Files:**\n",
        "- `results/evaluation_metrics/input_asr_metrics.csv`\n",
        "- `results/evaluation_metrics/output_asr_metrics.csv`\n",
        "\n",
        "**Output:**\n",
        "- A bar chart saved as `results/evaluation_metrics/asr_comparison_chart.png`\n"
      ],
      "metadata": {
        "id": "U7dkMDTZIYQR"
      },
      "id": "U7dkMDTZIYQR"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# üìä STEP 7 ‚Äî ASR Metrics Visualization (Input vs Output)\n",
        "# ==========================================================\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "def load_asr_summary(csv_path: Path):\n",
        "    \"\"\"Load ASR metrics and compute average WER, CER, SER.\"\"\"\n",
        "    if not csv_path.exists():\n",
        "        print(f\"‚ö†Ô∏è Missing file: {csv_path}\")\n",
        "        return {\"WER\": None, \"CER\": None, \"SER\": None}\n",
        "    df = pd.read_csv(csv_path)\n",
        "    return {\n",
        "        \"WER\": df[\"WER\"].mean(),\n",
        "        \"CER\": df[\"CER\"].mean(),\n",
        "        \"SER\": df[\"SER\"].mean()\n",
        "    }\n",
        "\n",
        "def plot_asr_comparison(\n",
        "    input_asr_csv: Path,\n",
        "    output_asr_csv: Path,\n",
        "    output_dir: Path\n",
        "):\n",
        "    \"\"\"\n",
        "    Compare average ASR metrics between input and output sides,\n",
        "    and save a side-by-side bar chart visualization.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_asr_csv : Path\n",
        "        CSV file containing input-side ASR metrics.\n",
        "    output_asr_csv : Path\n",
        "        CSV file containing output-side ASR metrics.\n",
        "    output_dir : Path\n",
        "        Directory where the visualization will be saved.\n",
        "    \"\"\"\n",
        "    # --- Load average metrics ---\n",
        "    input_metrics = load_asr_summary(input_asr_csv)\n",
        "    output_metrics = load_asr_summary(output_asr_csv)\n",
        "\n",
        "    # --- Prepare data for plotting ---\n",
        "    metrics = [\"WER\", \"CER\", \"SER\"]\n",
        "    input_vals = [input_metrics[m] for m in metrics]\n",
        "    output_vals = [output_metrics[m] for m in metrics]\n",
        "\n",
        "    # --- Plot chart ---\n",
        "    fig, ax = plt.subplots(figsize=(7, 5))\n",
        "    x = range(len(metrics))\n",
        "    width = 0.35\n",
        "\n",
        "    ax.bar([i - width/2 for i in x], input_vals, width, label=\"Input ASR\", alpha=0.8)\n",
        "    ax.bar([i + width/2 for i in x], output_vals, width, label=\"Output ASR\", alpha=0.8)\n",
        "\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(metrics)\n",
        "    ax.set_ylabel(\"Error Rate\")\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_title(\"ASR Performance Comparison ‚Äî Input vs Output\")\n",
        "    ax.legend()\n",
        "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # --- Save chart ---\n",
        "    save_path = Path(output_dir) / \"asr_comparison_chart.png\"\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"üìä ASR comparison chart saved to: {save_path}\")\n",
        "    return save_path\n",
        "\n"
      ],
      "metadata": {
        "id": "314K3oxCIaDu"
      },
      "id": "314K3oxCIaDu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Step 8 ‚Äî LLM Question Answering (Blood Pressure Analysis)\n",
        "\n",
        "This step defines the **LLM-based question-answering function** for the HealthTequity Voice Pipeline.  \n",
        "The model uses the translated English questions from the Spanish audio and answers them using the provided synthetic blood-pressure dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è How It Works\n",
        "- The dataset is passed to the model as a **CSV text block** (context).  \n",
        "- Each question (translated from Spanish ‚Üí English) is analyzed in context.  \n",
        "- The LLM returns a structured **JSON response** containing:\n",
        "  - `\"answer\"` ‚Äî a natural-language English explanation.\n",
        "  - `\"computed_fields\"` ‚Äî optional numeric values or summaries used in reasoning.\n",
        "\n",
        "---\n",
        "\n",
        "### üìö Behavior Rules\n",
        "- The LLM can only use the dataset for factual answers.  \n",
        "- It can cite external information **only** when describing ‚Äúnormal‚Äù blood pressure ranges.  \n",
        "- All outputs follow a conversational, user-friendly tone.\n",
        "\n",
        "---\n",
        "\n",
        "### üß© Usage Example\n",
        "```python\n",
        "csv_text = (CSV_DIR / \"synthetic_bp_one_person.csv\").read_text()\n",
        "q = \"What is my average blood pressure this week?\"\n",
        "response = ask_gpt(q, csv_text)\n",
        "print(response[\"answer\"])\n"
      ],
      "metadata": {
        "id": "eu9pvHxqJjH2"
      },
      "id": "eu9pvHxqJjH2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "{\n",
        "  \"answer\": \"Your average systolic pressure this week was 118 mm Hg and your diastolic pressure was 77 mm Hg.\",\n",
        "  \"computed_fields\": {\"systolic_avg\": 118, \"diastolic_avg\": 77}\n",
        "}\n"
      ],
      "metadata": {
        "id": "uvo1FsVgJyk7"
      },
      "id": "uvo1FsVgJyk7"
    },
    {
      "cell_type": "markdown",
      "id": "f4477ecb",
      "metadata": {
        "id": "f4477ecb"
      },
      "source": [
        "\n",
        "## Step 3 ‚Äì LLM Question Answering <a id=\"qa\"></a>\n",
        "This section queries an LLM with English questions derived from the ASR+translation step and provides answers based on a tabular blood-pressure dataset.\n",
        "\n",
        "**Inputs**: A CSV file with synthetic blood-pressure records.  \n",
        "**Outputs**: English answers and associated computed fields.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85f89584",
      "metadata": {
        "id": "85f89584"
      },
      "outputs": [],
      "source": [
        "\n",
        "#  ==========================================================\n",
        "# üß† STEP 8 ‚Äî LLM Question Answering (Blood Pressure Analysis)\n",
        "# ==========================================================\n",
        "import json\n",
        "\n",
        "def ask_gpt(question_en: str, csv_block: str) -> dict:\n",
        "    \"\"\"\n",
        "    Query the LLM with an English question and the CSV dataset context.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    question_en : str\n",
        "        English question derived from Spanish transcription.\n",
        "    csv_block : str\n",
        "        CSV content as a text block for in-context grounding.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Dictionary containing:\n",
        "        - \"answer\" : str ‚Äî model's English response\n",
        "        - \"computed_fields\" : dict ‚Äî optional numeric details\n",
        "    \"\"\"\n",
        "\n",
        "    system_prompt = \"\"\"\n",
        "You are a careful and detail-oriented data analyst.\n",
        "\n",
        "You are given a synthetic blood pressure dataset in CSV format. It contains readings for one individual over the last 30 consecutive days, with the following columns:\n",
        "\n",
        "- date\n",
        "- age\n",
        "- sex\n",
        "- systolic_mmHg\n",
        "- diastolic_mmHg\n",
        "\n",
        "Use only the data in the CSV to answer all questions, except when normal blood pressure ranges are requested ‚Äî in those cases, you may use external references but must cite your source.\n",
        "\n",
        "---\n",
        "\n",
        "üß† Interpretation Guidelines:\n",
        "\n",
        "- \"Today\" refers to the most recent date in the dataset.\n",
        "- \"Yesterday\" means the date before \"today\" in the dataset.\n",
        "- \"Last week\" or \"last month\" refer to 7- or 30-day windows before \"today\".\n",
        "- If a date or range is unavailable, clearly say so.\n",
        "- Use conversational date formats like ‚ÄúOctober 12‚Äù instead of numeric ones.\n",
        "\n",
        "---\n",
        "\n",
        "üí¨ Answer Style:\n",
        "- Use natural, conversational English.\n",
        "- Address the user as ‚Äúyou‚Äù.\n",
        "- Respond clearly and concisely.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ Response Format:\n",
        "Always return valid JSON with this structure:\n",
        "\n",
        "{\n",
        "  \"answer\": \"<English answer>\",\n",
        "  \"computed_fields\": { \"numeric values used\" }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "    user_prompt = (\n",
        "        f\"CSV Dataset:\\n{csv_block}\\n\\n\"\n",
        "        f\"Question:\\n{question_en}\\n\\n\"\n",
        "        \"Please analyze the data and respond strictly in valid JSON format as defined above.\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            temperature=0,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "                {\"role\": \"user\", \"content\": user_prompt.strip()},\n",
        "            ],\n",
        "        )\n",
        "        answer_text = response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenAI API error: {e}\")\n",
        "        return {\"answer\": \"Error: failed to retrieve response.\", \"computed_fields\": {}}\n",
        "\n",
        "    # Safely handle JSON parsing\n",
        "    try:\n",
        "        result = json.loads(answer_text)\n",
        "        if not isinstance(result, dict):\n",
        "            raise ValueError(\"Invalid JSON structure.\")\n",
        "    except Exception:\n",
        "        result = {\"answer\": answer_text, \"computed_fields\": {}}\n",
        "\n",
        "    return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîä Step 9 ‚Äî Spanish Translation & Text-to-Speech (Optimized)\n",
        "\n",
        "This module converts each English answer from the LLM into natural-sounding **Spanish audio**.  \n",
        "It provides two flexible layers:\n",
        "\n",
        "1. **Translation (English ‚Üí Spanish)** ‚Äî by default uses **OpenAI GPT-4o-mini**,  \n",
        "   but automatically falls back to `googletrans` if an API key is not available.\n",
        "2. **Text-to-Speech (Spanish ‚Üí Audio)** ‚Äî uses **gTTS + pydub** (free and Colab-friendly).\n",
        "\n",
        "All outputs are saved as `.wav` files in the `results/tts_audio/` directory.\n",
        "\n",
        "---\n",
        "\n",
        "### üì• Input / üì§ Output\n",
        "\n",
        "| Step | Input | Output |\n",
        "|------|--------|---------|\n",
        "| Translation | English text | Spanish text |\n",
        "| TTS | Spanish text | Spoken Spanish `.wav` file |\n"
      ],
      "metadata": {
        "id": "ma2u3TE7ML5w"
      },
      "id": "ma2u3TE7ML5w"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p4RdoHuOlzBf"
      },
      "id": "p4RdoHuOlzBf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f48ecab5",
      "metadata": {
        "id": "f48ecab5"
      },
      "outputs": [],
      "source": [
        "# ==========================================================\n",
        "# üîä STEP 9 ‚Äî SPANISH TRANSLATION & TEXT-TO-SPEECH (OPTIMIZED)\n",
        "# ==========================================================\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def translate_to_spanish(text_en: str) -> str:\n",
        "    \"\"\"\n",
        "    Translate English text to Spanish.\n",
        "    Uses OpenAI GPT if available, otherwise falls back to googletrans.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text_en : str\n",
        "        English text to translate.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Spanish translation.\n",
        "    \"\"\"\n",
        "    # --- 1Ô∏è‚É£ Try OpenAI translation first ---\n",
        "    try:\n",
        "        if \"client\" in globals():\n",
        "            prompt = (\n",
        "                \"Translate the following English medical answer into clear, \"\n",
        "                \"natural Spanish:\\n\\n\" + text_en\n",
        "            )\n",
        "            resp = client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                temperature=0,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            )\n",
        "            return resp.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è OpenAI translation failed: {e}\")\n",
        "\n",
        "    # --- 2Ô∏è‚É£ Fallback: googletrans (free) ---\n",
        "    try:\n",
        "        from googletrans import Translator\n",
        "        translator = Translator()\n",
        "        return translator.translate(text_en, src=\"en\", dest=\"es\").text\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è googletrans fallback failed: {e}\")\n",
        "        return \"(translation unavailable)\"\n",
        "\n",
        "def text_to_speech_spanish(text_es: str, out_wav_path: Path):\n",
        "    \"\"\"\n",
        "    Generate Spanish TTS audio (free fallback using gTTS + pydub).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text_es : str\n",
        "        Spanish text to synthesize.\n",
        "    out_wav_path : Path\n",
        "        Destination path for the WAV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from gtts import gTTS\n",
        "        from pydub import AudioSegment\n",
        "\n",
        "        out_wav_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        tmp_mp3 = out_wav_path.with_suffix(\".mp3\")\n",
        "\n",
        "        # Generate MP3 and convert to WAV\n",
        "        gTTS(text_es, lang=\"es\").save(tmp_mp3)\n",
        "        AudioSegment.from_mp3(tmp_mp3).export(out_wav_path, format=\"wav\")\n",
        "        os.remove(tmp_mp3)\n",
        "\n",
        "        print(f\"‚úÖ Spanish TTS saved ‚Üí {out_wav_path.name}\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"TTS generation failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Step 10 ‚Äî Full Pipeline Integration\n",
        "\n",
        "This step executes the **complete HealthTequity Voice Pipeline**, combining all previous components into a single, reproducible workflow.\n",
        "\n",
        "The pipeline processes spoken Spanish questions, interprets them through an AI-driven analytics system, and produces accurate Spanish spoken answers grounded in tabular blood-pressure data.\n",
        "\n",
        "---\n",
        "\n",
        "### üß© Workflow Overview\n",
        "\n",
        "| Step | Process | Description | Output |\n",
        "|------|----------|--------------|---------|\n",
        "| 1Ô∏è‚É£ | **Input ASR + Translation** | Transcribes each Spanish question and translates it into English. | `audio_translations.csv` |\n",
        "| 2Ô∏è‚É£ | **Input ASR Evaluation** | Compares Whisper transcriptions with ground-truth text to compute WER, CER, and SER. | `input_asr_metrics.csv` |\n",
        "| 3Ô∏è‚É£ | **CSV Grounding** | Loads the synthetic blood-pressure dataset as the context for LLM reasoning. | In-memory CSV string |\n",
        "| 4Ô∏è‚É£ | **LLM Question Answering** | Uses GPT to analyze the dataset and answer each English question. | English answer text |\n",
        "| 5Ô∏è‚É£ | **Spanish Translation + TTS** | Converts each English answer into natural Spanish and generates speech audio. | `results/tts_audio/*.wav` |\n",
        "| 6Ô∏è‚É£ | **Output ASR Evaluation** | Transcribes TTS-generated Spanish audio and compares it to ground-truth Spanish answers. | `output_asr_metrics.csv` |\n",
        "| 7Ô∏è‚É£ | **Visualization** | Displays a side-by-side comparison of WER, CER, and SER for input vs. output ASR. | Matplotlib chart |\n",
        "\n",
        "---\n",
        "\n",
        "### üìÅ Input Requirements\n",
        "- **`data/synthetic_csv/synthetic_bp_one_person.csv`** ‚Äî Blood-pressure dataset  \n",
        "- **`data/Spanish_audio/`** ‚Äî Spanish question audio files  \n",
        "- **`data/synthetic_csv/ground_truth.csv`** ‚Äî Ground-truth transcription for ASR evaluation  \n",
        "\n",
        "Reviewers may replace these files with their own data to test other datasets or audio samples.\n",
        "\n",
        "---\n",
        "\n",
        "### üíæ Output Artifacts\n",
        "All generated files are automatically stored in the following directories:\n",
        "- **Transcriptions & LLM results:** `results/llm_outputs/`\n",
        "- **Evaluation metrics:** `results/evaluation_metrics/`\n",
        "- **Spanish speech audio:** `results/tts_audio/`\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è Configuration\n",
        "You can select different Whisper models for accuracy/speed trade-offs using the parameter:\n",
        "```python\n",
        "whisper_model_size=\"base\"  # options: \"tiny\", \"base\", \"small\", \"medium\", \"large\"\n"
      ],
      "metadata": {
        "id": "CszJZQ6ZOPbk"
      },
      "id": "CszJZQ6ZOPbk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "852067e6",
      "metadata": {
        "id": "852067e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ead418f-accf-4594-d7b6-e42bc79caa3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "üéôÔ∏è STEP 1: Input ASR + Translation (Spanish ‚Üí English)\n",
            "==============================\n",
            "üéß Loading Whisper model: base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Processing 9 audio files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1303/1303 [00:04<00:00, 325.25frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ q10_es.wav ‚Üí processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 477/477 [00:03<00:00, 156.88frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ q2_es.wav ‚Üí processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 427/427 [00:02<00:00, 166.57frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ q3_es.wav ‚Üí processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 573/573 [00:02<00:00, 207.54frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ q4_es.wav ‚Üí processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 328/328 [00:03<00:00, 83.21frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ q5_es.wav ‚Üí processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1461/1461 [00:04<00:00, 346.44frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ q6_es.wav ‚Üí processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 739/739 [00:02<00:00, 269.37frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ q7_es.wav ‚Üí processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 516/516 [00:02<00:00, 192.25frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ q8_es.wav ‚Üí processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 458/458 [00:02<00:00, 176.23frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ q9_es.wav ‚Üí processed\n",
            "\n",
            "üíæ Results saved to: /content/drive/MyDrive/HealthTequity-LLM/results/llm_outputs/audio_translations.csv\n",
            "\n",
            "==============================\n",
            "üìä STEP 2: Evaluate Input ASR (WER / CER / SER)\n",
            "==============================\n",
            "üéß Loading Whisper model: base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Evaluating 10 audio files...\n",
            "‚ö†Ô∏è Missing audio file: q1_es.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 477/477 [00:03<00:00, 134.15frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ q2_es.wav ‚Üí WER=0.0, CER=0.0, SER=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 427/427 [00:03<00:00, 141.22frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ q3_es.wav ‚Üí WER=0.0, CER=0.0, SER=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 573/573 [00:02<00:00, 203.56frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ q4_es.wav ‚Üí WER=0.0714, CER=0.0128, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 328/328 [00:02<00:00, 129.30frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ q5_es.wav ‚Üí WER=0.0, CER=0.0, SER=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1461/1461 [00:04<00:00, 300.37frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ q6_es.wav ‚Üí WER=0.0968, CER=0.0595, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 739/739 [00:03<00:00, 191.49frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ q7_es.wav ‚Üí WER=0.0, CER=0.0, SER=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 516/516 [00:02<00:00, 193.63frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ q8_es.wav ‚Üí WER=0.0, CER=0.0, SER=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 458/458 [00:03<00:00, 149.34frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ q9_es.wav ‚Üí WER=0.0, CER=0.0, SER=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1303/1303 [00:04<00:00, 287.91frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ q10_es.wav ‚Üí WER=0.0714, CER=0.0121, SER=1\n",
            "\n",
            "üíæ ASR metrics saved to: /content/drive/MyDrive/HealthTequity-LLM/results/evaluation_metrics/input_asr_metrics.csv\n",
            "üìä Average WER=0.027, CER=0.009, SER=0.333\n",
            "\n",
            "==============================\n",
            "üìà STEP 3: LLM Question Answering\n",
            "==============================\n",
            "‚úÖ Spanish TTS saved ‚Üí answer_1_es.wav\n",
            "‚úÖ Spanish TTS saved ‚Üí answer_2_es.wav\n",
            "‚úÖ Spanish TTS saved ‚Üí answer_3_es.wav\n",
            "‚úÖ Spanish TTS saved ‚Üí answer_4_es.wav\n",
            "‚úÖ Spanish TTS saved ‚Üí answer_5_es.wav\n",
            "‚úÖ Spanish TTS saved ‚Üí answer_6_es.wav\n",
            "‚úÖ Spanish TTS saved ‚Üí answer_7_es.wav\n",
            "‚úÖ Spanish TTS saved ‚Üí answer_8_es.wav\n",
            "‚úÖ Spanish TTS saved ‚Üí answer_9_es.wav\n",
            "‚úÖ Saved final results to: /content/drive/MyDrive/HealthTequity-LLM/results/llm_outputs/final_pipeline_results.csv\n",
            "\n",
            "==============================\n",
            "üß† STEP 4: Evaluate Output ASR (WER / CER / SER)\n",
            "==============================\n",
            "üéß Loading Whisper model: base\n",
            "üîç Evaluating 9 audio files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8366/8366 [00:28<00:00, 297.45frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_1_es.wav ‚Üí WER=2.1892, CER=0.7607, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16216/16216 [00:51<00:00, 316.15frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_2_es.wav ‚Üí WER=1.9787, CER=0.547, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6703/6703 [00:17<00:00, 377.60frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_3_es.wav ‚Üí WER=0.4416, CER=0.1766, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3484/3484 [00:11<00:00, 296.51frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_4_es.wav ‚Üí WER=1.0889, CER=0.2934, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2066/2066 [00:17<00:00, 121.52frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_5_es.wav ‚Üí WER=0.375, CER=0.4762, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5090/5090 [00:46<00:00, 109.38frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_6_es.wav ‚Üí WER=0.127, CER=0.1835, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:19<00:00, 301.75frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_7_es.wav ‚Üí WER=0.2286, CER=0.1885, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1401/1401 [00:03<00:00, 418.31frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_8_es.wav ‚Üí WER=0.1176, CER=0.1889, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5623/5623 [00:18<00:00, 296.32frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_9_es.wav ‚Üí WER=1.4032, CER=0.3877, SER=1\n",
            "\n",
            "üíæ ASR metrics saved to: /content/drive/MyDrive/HealthTequity-LLM/results/evaluation_metrics/output_asr_metrics.csv\n",
            "üìä Average WER=0.883, CER=0.356, SER=1.000\n",
            "\n",
            "==============================\n",
            "üìä STEP 5: Visualize ASR Comparison\n",
            "==============================\n",
            "üìä ASR comparison chart saved to: /content/drive/MyDrive/HealthTequity-LLM/results/evaluation_metrics/asr_comparison_chart.png\n",
            "\n",
            "‚úÖ Full pipeline completed successfully.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def run_full_pipeline(csv_path: Path, audio_folder: Path, whisper_model_size: str = \"base\"):\n",
        "    \"\"\"\n",
        "    Execute the full HealthTequity voice analysis pipeline.\n",
        "    Includes:\n",
        "      1Ô∏è‚É£ Input-side ASR & translation (Spanish ‚Üí English)\n",
        "      2Ô∏è‚É£ ASR evaluation (WER / CER / SER)\n",
        "      3Ô∏è‚É£ LLM-driven Q&A on the blood pressure dataset\n",
        "      4Ô∏è‚É£ Spanish translation + TTS output\n",
        "      5Ô∏è‚É£ Output-side ASR evaluation (WER / CER / SER)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    csv_path : Path\n",
        "        Path to the synthetic blood-pressure CSV.\n",
        "    audio_folder : Path\n",
        "        Directory containing input Spanish .wav files.\n",
        "    whisper_model_size : str, optional\n",
        "        Whisper model size (default: \"base\").\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Summary dictionary with key file paths for inspection.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"üéôÔ∏è STEP 1: Input ASR + Translation (Spanish ‚Üí English)\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    trans_csv = LLM_OUT / \"audio_translations.csv\"\n",
        "    _ = process_and_translate_audio(audio_folder, trans_csv, model_size=whisper_model_size)\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"üìä STEP 2: Evaluate Input ASR (WER / CER / SER)\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    gt_csv = CSV_DIR / \"ground_truth.csv\"\n",
        "    asr_csv = EVAL_DIR / \"input_asr_metrics.csv\"\n",
        "    _ = evaluate_asr_whisper(\n",
        "        gt_csv=gt_csv,\n",
        "        audio_folder=audio_folder,\n",
        "        output_csv=asr_csv,\n",
        "        model_size=whisper_model_size,\n",
        "        gt_text_col=\"ground_truth\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"üìà STEP 3: LLM Question Answering\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    df_bp = pd.read_csv(csv_path)\n",
        "    csv_block = df_bp.to_csv(index=False)\n",
        "    trans_df = pd.read_csv(trans_csv)\n",
        "\n",
        "    results = []\n",
        "    for i, row in trans_df.iterrows():\n",
        "        q_en = row[\"english_translation\"]\n",
        "        ans = ask_gpt(q_en, csv_block)\n",
        "        ans_en = ans.get(\"answer\", \"\").strip()\n",
        "        ans_es = translate_to_spanish(ans_en)\n",
        "\n",
        "        # Generate Spanish TTS output\n",
        "        out_wav = TTS_DIR / f\"answer_{i + 1}_es.wav\"\n",
        "        text_to_speech_spanish(ans_es, out_wav)\n",
        "\n",
        "        results.append({\n",
        "            \"question_number\": i + 1,\n",
        "            \"audio_file_in\": row[\"audio_file\"],\n",
        "            \"spanish_question\": row[\"spanish_transcription\"],\n",
        "            \"english_question\": q_en,\n",
        "            \"english_answer\": ans_en,\n",
        "            \"spanish_answer\": ans_es,\n",
        "            \"audio_file\": str(out_wav),  # ‚úÖ unified column for ASR evaluation\n",
        "            \"computed_fields\": json.dumps(ans.get(\"computed_fields\", {}))\n",
        "        })\n",
        "\n",
        "    final_csv = LLM_OUT / \"final_pipeline_results.csv\"\n",
        "    pd.DataFrame(results).to_csv(final_csv, index=False)\n",
        "    print(f\"‚úÖ Saved final results to: {final_csv}\")\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"üß† STEP 4: Evaluate Output ASR (WER / CER / SER)\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    output_asr_csv = EVAL_DIR / \"output_asr_metrics.csv\"\n",
        "    _ = evaluate_asr_whisper(\n",
        "        gt_csv=final_csv,\n",
        "        audio_folder=TTS_DIR,\n",
        "        output_csv=output_asr_csv,\n",
        "        model_size=whisper_model_size,\n",
        "        gt_text_col=\"spanish_answer\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"üìä STEP 5: Visualize ASR Comparison\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    plot_asr_comparison(\n",
        "        input_asr_csv=asr_csv,\n",
        "        output_asr_csv=output_asr_csv,\n",
        "        output_dir=EVAL_DIR\n",
        "    )\n",
        "\n",
        "    print(\"\\n‚úÖ Full pipeline completed successfully.\")\n",
        "    return {\n",
        "        \"transcriptions_csv\": str(trans_csv),\n",
        "        \"input_asr_metrics_csv\": str(asr_csv),\n",
        "        \"final_pipeline_csv\": str(final_csv),\n",
        "        \"output_asr_metrics_csv\": str(output_asr_csv)\n",
        "    }\n",
        "\n",
        "\n",
        "# Example (not auto-run in submission)\n",
        "bp_csv = CSV_DIR / \"synthetic_bp_one_person.csv\"\n",
        "_ = run_full_pipeline(bp_csv, AUDIO_DIR, whisper_model_size=\"base\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}